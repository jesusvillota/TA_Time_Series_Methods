{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe316ce2",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 600px; margin: 20px auto 22px; padding: 0px; border-radius: 18px; border: 1px solid #e5e7eb; background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%); box-shadow: 0 8px 26px rgba(0,0,0,0.06); overflow: hidden;\">\n",
    "\n",
    "  <!-- Banner Header -->\n",
    "  <div style=\"padding: 34px 32px 14px; text-align: center; line-height: 1.38;\">\n",
    "    <div style=\"font-size: 13px; letter-spacing: 0.14em; text-transform: uppercase; color: #6b7280; font-weight: bold; margin-bottom: 5px;\">\n",
    "      Session #1\n",
    "    </div>\n",
    "    <div style=\"font-size: 29px; font-weight: 800; color: #14276c; margin-bottom: 4px;\">\n",
    "      Cleaning Time Series Data\n",
    "    </div>\n",
    "    <div style=\"font-size: 16.5px; color: #374151; font-style: italic; margin-bottom: 0;\">\n",
    "      Advanced Training School: Methods for Time Series\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Logo Section -->\n",
    "  <div style=\"background: none; text-align: center; margin: 30px 0 10px;\">\n",
    "    <img src=\"https://www.cemfi.es/images/Logo-Azul.png\" alt=\"CEMFI Logo\" style=\"width: 158px; filter: drop-shadow(0 2px 12px rgba(56,84,156,0.05)); margin-bottom: 0;\">\n",
    "  </div>\n",
    "\n",
    "  <!-- Name -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1.22em; font-weight: bold; margin-bottom: 0px;\">\n",
    "    Jesus Villota Miranda © 2025\n",
    "  </div>\n",
    "\n",
    "  <!-- Contact info -->\n",
    "  <div style=\"font-family: 'Times New Roman', Times, serif; color: #38549c; text-align: center; font-size: 1em; margin-top: 7px; margin-bottom: 20px;\">\n",
    "    <a href=\"mailto:jesus.villota@cemfi.edu.es\" style=\"color: #38549c; text-decoration: none; margin-right:8px;\" title=\"Email\">\n",
    "      jesus.villota@cemfi.edu.es\n",
    "    </a>\n",
    "    <span style=\"color:#9fa7bd;\">|</span>\n",
    "    <a href=\"https://www.linkedin.com/in/jesusvillotamiranda/\" target=\"_blank\" style=\"color: #38549c; text-decoration: none; margin-left:7px;\" title=\"LinkedIn\">\n",
    "      LinkedIn\n",
    "    </a>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a385a04c",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1. Load and clean raw financial and economic time series data from FRED\n",
    "2. Convert dates to proper Stata time series formats\n",
    "3. Create standard transformations (logs, returns, growth rates)\n",
    "4. Handle missing values and data quality issues\n",
    "5. Prepare cleaned datasets ready for time series analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255eb84",
   "metadata": {},
   "source": [
    "<style>\n",
    "h2.styled-header {\n",
    "    max-width: 600px;\n",
    "    margin: 20px auto 22px !important;\n",
    "    padding: 20px 32px !important;\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid #e5e7eb;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%);\n",
    "    box-shadow: 0 8px 26px rgba(0,0,0,0.06);\n",
    "    overflow: hidden;\n",
    "    text-align: center;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 800 !important;\n",
    "    color: #14276c !important;\n",
    "    margin-bottom: 8px !important;\n",
    "    margin-top: 0 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2 class=\"styled-header\">1) Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63edc17",
   "metadata": {},
   "source": [
    "This notebook processes raw time series data downloaded from FRED (Federal Reserve Economic Data) and prepares it for analysis in subsequent sessions.\n",
    "\n",
    "**Data Sources:**\n",
    "- Raw CSV files are stored in `data/raw/`\n",
    "- Processed Stata `.dta` files will be saved to `data/processed/`\n",
    "- All data is publicly available from FRED\n",
    "\n",
    "**Processing Steps:**\n",
    "1. Load raw CSV files\n",
    "2. Parse and convert dates to Stata date formats\n",
    "3. Create standard transformations (logs, returns, growth rates)\n",
    "4. Handle missing values appropriately\n",
    "5. Set up time series structure\n",
    "6. Add variable labels\n",
    "7. Save processed datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4d67f",
   "metadata": {},
   "source": [
    "<style>\n",
    "h2.styled-header {\n",
    "    max-width: 600px;\n",
    "    margin: 20px auto 22px !important;\n",
    "    padding: 20px 32px !important;\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid #e5e7eb;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%);\n",
    "    box-shadow: 0 8px 26px rgba(0,0,0,0.06);\n",
    "    overflow: hidden;\n",
    "    text-align: center;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 800 !important;\n",
    "    color: #14276c !important;\n",
    "    margin-bottom: 8px !important;\n",
    "    margin-top: 0 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2 class=\"styled-header\">2) Setup</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf8df5",
   "metadata": {},
   "source": [
    "## Why Data Cleaning Matters in Time Series Analysis\n",
    "\n",
    "Before diving into the code, it's important to understand why data cleaning is particularly critical for time series analysis:\n",
    "\n",
    "**Time Series-Specific Challenges:**\n",
    "1. **Temporal Dependencies**: Unlike cross-sectional data, time series observations are ordered and dependent. Missing or incorrectly ordered data can break these dependencies.\n",
    "2. **Frequency Consistency**: Financial and economic data come at different frequencies (daily, monthly, quarterly). Misaligned frequencies can lead to spurious relationships.\n",
    "3. **Stationarity Requirements**: Many time series models require stationary data (constant mean, variance). Proper transformations (logs, differences) are essential.\n",
    "4. **Date Handling**: Time series models rely on proper temporal indexing. Incorrect date formats can invalidate all subsequent analysis.\n",
    "\n",
    "**Common Issues We'll Address:**\n",
    "- Missing values (weekends, holidays in financial data)\n",
    "- Date format standardization (FRED uses YYYY-MM-DD)\n",
    "- Non-stationarity (we'll create log returns and first differences)\n",
    "- Mixed frequencies (combining quarterly GDP with monthly CPI)\n",
    "- Variable scaling (creating percentage returns, growth rates)\n",
    "\n",
    "**Transformations We'll Apply:**\n",
    "- **Log transformation**: For prices and levels, $\\ln(P_t)$, to stabilize variance and interpret changes as percentage changes\n",
    "- **First differences**: $\\Delta x_t = x_t - x_{t-1}$, to achieve stationarity\n",
    "- **Returns**: $r_t = \\ln(P_t/P_{t-1})$, the standard measure in finance\n",
    "\n",
    "This notebook will prepare three datasets for subsequent time series analysis:\n",
    "1. **S&P 500**: Daily stock returns for ARMA and GARCH models\n",
    "2. **EUR/USD**: Daily exchange rates for unit root testing\n",
    "3. **Macro-Finance**: Monthly economic indicators for VAR analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ec75c",
   "metadata": {},
   "source": [
    "## Understanding the Setup Commands\n",
    "\n",
    "Before we begin processing data, we need to prepare our Stata environment. Each setup command serves a specific purpose:\n",
    "\n",
    "### The `clear all` Command\n",
    "\n",
    "**Syntax:** `clear all`\n",
    "\n",
    "**What it does:** Removes everything from Stata's memory:\n",
    "- All data currently loaded\n",
    "- All value labels\n",
    "- All matrices\n",
    "- All scalars and macros\n",
    "- Closes any open graph windows\n",
    "\n",
    "**Why we use it:** \n",
    "- Ensures a clean slate - no leftover data from previous sessions\n",
    "- Prevents variable name conflicts\n",
    "- Essential for reproducibility - ensures the script runs the same way every time\n",
    "- Best practice: always start analysis scripts with `clear all`\n",
    "\n",
    "**Alternative:** `clear` (only clears data, not everything)\n",
    "\n",
    "---\n",
    "\n",
    "### The `set more off` Command\n",
    "\n",
    "**Syntax:** `set more off`\n",
    "\n",
    "**What it does:** Disables Stata's pagination feature\n",
    "\n",
    "**Why we use it:**\n",
    "- By default, Stata pauses output when the Results window is full, showing `--more--`\n",
    "- In scripts and notebooks, we want continuous output without manual intervention\n",
    "- Makes logs and output easier to read\n",
    "- Standard practice for automated workflows\n",
    "\n",
    "**Alternative:** `set more on` (re-enables pagination, useful for interactive sessions)\n",
    "\n",
    "**Note:** In modern Stata versions and notebooks, this is less critical but still good practice.\n",
    "\n",
    "---\n",
    "\n",
    "### The `cd` Command (Change Directory)\n",
    "\n",
    "**Syntax:** `cd \"path/to/directory\"`\n",
    "\n",
    "**What it does:** Changes the current working directory\n",
    "\n",
    "**Why we use it:**\n",
    "- All relative file paths are interpreted from the working directory\n",
    "- `cd \"../..\"` moves up two levels from `session_1/notebooks/` to the project root\n",
    "- Ensures we can access `data/raw/` and save to `data/processed/`\n",
    "\n",
    "**Path Navigation:**\n",
    "- `..` means parent directory (one level up)\n",
    "- `../..` means two levels up\n",
    "- `/` for absolute paths (from root)\n",
    "- `./` means current directory (optional)\n",
    "\n",
    "**Best Practice:** \n",
    "- Use relative paths from a consistent project root\n",
    "- Makes code portable across different computers\n",
    "- Avoids hardcoded absolute paths like `/Users/yourname/...`\n",
    "\n",
    "**Checking your location:** Use `pwd` (print working directory) to see where you are\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Order Matters\n",
    "\n",
    "1. **First**: `clear all` - clean the workspace\n",
    "2. **Second**: `set more off` - configure output behavior  \n",
    "3. **Third**: `cd` - set location for file operations\n",
    "\n",
    "This ensures we start fresh, configure our environment, then position ourselves correctly for data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5289fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "/Users/jesusvillotamiranda/Library/CloudStorage/OneDrive-UniversidaddeLaRioja/Gi\n",
      "> tHub/Repository/TA_Time_Series_Methods\n",
      "> tHub/Repository/TA_Time_Series_Methods\n"
     ]
    }
   ],
   "source": [
    "* Setup\n",
    "clear all\n",
    "set more off\n",
    "* Set working directory to project root (from notebooks folder)\n",
    "cd \"../..\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e12bb7",
   "metadata": {},
   "source": [
    "<style>\n",
    "h2.styled-header {\n",
    "    max-width: 600px;\n",
    "    margin: 20px auto 22px !important;\n",
    "    padding: 20px 32px !important;\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid #e5e7eb;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%);\n",
    "    box-shadow: 0 8px 26px rgba(0,0,0,0.06);\n",
    "    overflow: hidden;\n",
    "    text-align: center;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 800 !important;\n",
    "    color: #14276c !important;\n",
    "    margin-bottom: 8px !important;\n",
    "    margin-top: 0 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2 class=\"styled-header\">3) S&P 500 Index Data</h2>\n",
    "\n",
    "Process daily S&P 500 index data. This will be used for ARMA modeling and volatility analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0415a37f",
   "metadata": {},
   "source": [
    "### Step 1: Importing CSV Data and Initial Inspection\n",
    "\n",
    "Before we can analyze data, we need to load it into Stata's memory. The S&P 500 data comes from FRED (Federal Reserve Economic Data) as a CSV file.\n",
    "\n",
    "**The `import delimited` Command**\n",
    "\n",
    "**Full Syntax:**\n",
    "```stata\n",
    "import delimited [using] filename [, options]\n",
    "```\n",
    "\n",
    "**Key Options:**\n",
    "- `clear`: Replace data currently in memory (required if data already loaded)\n",
    "- `delimiter(\"char\")`: Specify delimiter (default is comma for .csv)\n",
    "- `varnames(#)`: Row number containing variable names (default = 1)\n",
    "- `stringcols(#)`: Force specific columns to be strings\n",
    "- `numericcols(#)`: Force specific columns to be numeric\n",
    "- `case(preserve|lower|upper)`: How to handle variable name case\n",
    "\n",
    "**Our Command:**\n",
    "```stata\n",
    "import delimited \"data/raw/SP500.csv\", clear\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Loads the CSV file from the `data/raw/` folder\n",
    "- `clear`: Replaces any existing data in memory\n",
    "- Automatically detects comma as delimiter (it's a .csv file)\n",
    "- Reads first row as variable names\n",
    "- Infers data types (string vs numeric) from the first few rows\n",
    "\n",
    "---\n",
    "\n",
    "**The `list` Command**\n",
    "\n",
    "**Syntax:** `list [varlist] [if] [in] [, options]`\n",
    "\n",
    "**Our use:** `list in 1/10`\n",
    "- `in 1/10`: Shows observations 1 through 10\n",
    "- Useful for quick preview of data structure\n",
    "- Alternative: `list in 1/5` (first 5), `list in -10/-1` (last 10)\n",
    "\n",
    "---\n",
    "\n",
    "**The `describe` Command**\n",
    "\n",
    "**Syntax:** `describe [varlist] [, options]`\n",
    "\n",
    "**What it shows:**\n",
    "- Number of observations and variables\n",
    "- Variable names, storage types (byte, int, long, float, double, str#)\n",
    "- Display format\n",
    "- Value labels (if any)\n",
    "- Variable labels (if any)\n",
    "\n",
    "**Why this matters for time series:**\n",
    "- Confirms we have the expected variables (date and price)\n",
    "- Checks data types (dates often import as strings and need conversion)\n",
    "- Shows the number of observations (determines sample size for analysis)\n",
    "\n",
    "**Expected Output:**\n",
    "- `observation_date`: likely string type (str10 or similar)\n",
    "- `sp500`: likely float or double (numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff05c906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(encoding automatically selected: ISO-8859-1)\n",
      "(2 vars, 2,609 obs)\n",
      "\n",
      "\n",
      "     +----------------------+\n",
      "     | observat~e     sp500 |\n",
      "     |----------------------|\n",
      "  1. | 2015-11-04   2102.31 |\n",
      "  2. | 2015-11-05   2099.93 |\n",
      "  3. | 2015-11-06    2099.2 |\n",
      "  4. | 2015-11-09   2078.58 |\n",
      "  5. | 2015-11-10   2081.72 |\n",
      "     |----------------------|\n",
      "  6. | 2015-11-11      2075 |\n",
      "  7. | 2015-11-12   2045.97 |\n",
      "  8. | 2015-11-13   2023.04 |\n",
      "  9. | 2015-11-16   2053.19 |\n",
      " 10. | 2015-11-17   2050.44 |\n",
      "     +----------------------+\n",
      "\n",
      "\n",
      "Contains data\n",
      " Observations:         2,609                  \n",
      "    Variables:             2                  \n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "observation_d~e str10   %10s                  \n",
      "sp500           float   %9.0g                 SP500\n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: \n",
      "     Note: Dataset has changed since last saved.\n"
     ]
    }
   ],
   "source": [
    "* Load S&P 500 raw data\n",
    "import delimited \"data/raw/SP500.csv\", clear\n",
    "\n",
    "* Display first few observations\n",
    "list in 1/10\n",
    "\n",
    "* Check data structure\n",
    "describe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590db9e",
   "metadata": {},
   "source": [
    "### Step 2: Converting String Dates to Stata Date Format\n",
    "\n",
    "One of the most critical steps in time series analysis is proper date handling. Stata has a sophisticated date system, but dates from CSV files typically import as strings.\n",
    "\n",
    "**Understanding Stata's Date System**\n",
    "\n",
    "Stata stores dates internally as **numbers**:\n",
    "- Daily dates: days since January 1, 1960\n",
    "  - January 1, 1960 = 0\n",
    "  - January 2, 1960 = 1\n",
    "  - December 31, 1959 = -1\n",
    "- This allows for easy date arithmetic (e.g., `date + 7` adds a week)\n",
    "\n",
    "**The Difference Between Storage and Display:**\n",
    "- **Storage format**: How Stata stores the date internally (always a number)\n",
    "- **Display format**: How Stata shows the date to you (e.g., \"01jan2020\", \"2020-01-01\")\n",
    "- The `%td`, `%tm`, `%tq` formats are display formats\n",
    "\n",
    "---\n",
    "\n",
    "**The `date()` Function**\n",
    "\n",
    "**Syntax:** `date(string_date, \"mask\")`\n",
    "\n",
    "**What it does:** Converts a string containing a date to Stata's numeric date format\n",
    "\n",
    "**Common Masks:**\n",
    "- `\"DMY\"`: Day-Month-Year (e.g., \"31-12-2020\" or \"31/12/2020\")\n",
    "- `\"MDY\"`: Month-Day-Year (e.g., \"12-31-2020\" or \"12/31/2020\")  \n",
    "- `\"YMD\"`: Year-Month-Day (e.g., \"2020-12-31\") ← **FRED uses this**\n",
    "- `\"MY\"`: Month-Year for monthly data\n",
    "- `\"Y\"`: Year only for annual data\n",
    "\n",
    "**Our case:**\n",
    "```stata\n",
    "gen date_numeric = date(observation_date, \"YMD\")\n",
    "```\n",
    "- FRED uses ISO format: \"YYYY-MM-DD\" (e.g., \"2020-03-15\")\n",
    "- `\"YMD\"` mask tells Stata to parse year first, then month, then day\n",
    "- Creates a new variable with the numeric date value\n",
    "- We keep the original to verify before dropping\n",
    "\n",
    "**Why we drop `observation_date`:**\n",
    "- The string version is no longer needed\n",
    "- Reduces memory usage\n",
    "- Prevents confusion (one date variable is clearer)\n",
    "\n",
    "---\n",
    "\n",
    "**The `format` Command**\n",
    "\n",
    "**Syntax:** `format varlist %fmt`\n",
    "\n",
    "**What it does:** Changes how variables are **displayed** (not stored)\n",
    "\n",
    "**Common Date Formats:**\n",
    "- `%td`: Daily dates (e.g., \"15mar2020\")\n",
    "- `%tw`: Weekly dates  \n",
    "- `%tm`: Monthly dates (e.g., \"2020m3\")\n",
    "- `%tq`: Quarterly dates (e.g., \"2020q1\")\n",
    "- `%ty`: Yearly dates\n",
    "\n",
    "**Our use:**\n",
    "```stata\n",
    "format date_numeric %td\n",
    "```\n",
    "- Displays the numeric value as a readable date\n",
    "- Doesn't change the underlying number\n",
    "- Makes verification easier\n",
    "\n",
    "---\n",
    "\n",
    "**The `rename` Command**\n",
    "\n",
    "**Syntax:** `rename old_varname new_varname`\n",
    "\n",
    "**Why we rename to `date`:**\n",
    "- Convention: the time variable should be called `date` (or `time`, `t`)\n",
    "- Required for `tsset` (we'll see this soon)\n",
    "- Keeps code consistent across datasets\n",
    "\n",
    "---\n",
    "\n",
    "**Handling Duplicates**\n",
    "\n",
    "**The `duplicates` Commands:**\n",
    "\n",
    "1. **`duplicates report varlist`**: Shows how many duplicates exist\n",
    "   - Non-duplicates: observations appearing once\n",
    "   - Duplicates: observations appearing multiple times\n",
    "   - Doesn't change data, just reports\n",
    "\n",
    "2. **`duplicates drop varlist, force`**: Removes duplicate observations\n",
    "   - Keeps first occurrence of each unique value\n",
    "   - `force`: Required when dropping based on varlist (not all variables)\n",
    "   - Critical for time series: each date should appear once\n",
    "\n",
    "**Why duplicates matter in time series:**\n",
    "- Time series models assume one observation per time period\n",
    "- Duplicates can indicate data errors\n",
    "- `tsset` will fail if duplicates exist\n",
    "\n",
    "---\n",
    "\n",
    "**The `sort` Command**\n",
    "\n",
    "**Syntax:** `sort varlist`\n",
    "\n",
    "**What it does:** Arranges observations in ascending order\n",
    "\n",
    "**Why we sort by date:**\n",
    "- Time series must be in chronological order\n",
    "- Many time series operations assume sorted data\n",
    "- Required before `tsset`\n",
    "- Makes visual inspection easier\n",
    "\n",
    "**Note:** `sort` is **stable** - observations with the same date value maintain their relative order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98bdb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "--------------------------------------\n",
      "   Copies | Observations       Surplus\n",
      "----------+---------------------------\n",
      "        1 |         2609             0\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "(0 observations are duplicates)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "* Parse dates from observation_date column (FRED format: YYYY-MM-DD)\n",
    "gen date_numeric = date(observation_date, \"YMD\")\n",
    "drop observation_date\n",
    "\n",
    "* Format as Stata daily date\n",
    "format date_numeric %td\n",
    "rename date_numeric date\n",
    "\n",
    "* Check for duplicates\n",
    "duplicates report date\n",
    "duplicates drop date, force\n",
    "\n",
    "* Sort by date\n",
    "sort date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19aa37",
   "metadata": {},
   "source": [
    "### Step 3: Variable Naming and Missing Value Assessment\n",
    "\n",
    "Now that we have proper dates, we need to rename our price variable meaningfully and assess data quality.\n",
    "\n",
    "**The `rename` Command for Data Variables**\n",
    "\n",
    "**Why descriptive names matter:**\n",
    "- `sp500` (from CSV) is ambiguous - is it open, close, high, low?\n",
    "- `sp500_close` clearly indicates this is the closing price\n",
    "- Future you (and collaborators) will thank you\n",
    "- Makes code self-documenting\n",
    "\n",
    "**Naming conventions:**\n",
    "- Use underscores to separate words: `sp500_close` not `sp500close`\n",
    "- Start with letters, not numbers\n",
    "- Avoid Stata reserved words: `if`, `in`, `by`, etc.\n",
    "- Be consistent: if you use `_close` for one series, use it for all\n",
    "\n",
    "---\n",
    "\n",
    "**Understanding Missing Values in Stata**\n",
    "\n",
    "**Stata's missing value system:**\n",
    "- Numeric missing: represented by `.` (dot)\n",
    "- Extended missing: `.a`, `.b`, ..., `.z` (27 types) for different reasons\n",
    "- String missing: `\"\"` (empty string)\n",
    "\n",
    "**Critical property:** Missing values are treated as **positive infinity** in comparisons!\n",
    "- `. > 1000000` evaluates to TRUE\n",
    "- Always use `missing()` function in conditions\n",
    "- Never use `if x != .` (this fails!)\n",
    "\n",
    "---\n",
    "\n",
    "**The `count` Command**\n",
    "\n",
    "**Syntax:** `count [if] [in]`\n",
    "\n",
    "**What it does:** Counts observations meeting a condition\n",
    "\n",
    "**Our use:**\n",
    "```stata\n",
    "count if missing(sp500_close)\n",
    "```\n",
    "\n",
    "**The `missing()` function:**\n",
    "- Returns 1 (true) if value is missing\n",
    "- Returns 0 (false) if value is not missing\n",
    "- Works with all missing codes: `.`, `.a`, `.b`, etc.\n",
    "- **Always use this instead of `== .`**\n",
    "\n",
    "**Why check for missing values:**\n",
    "- Missing values are expected in financial data (weekends, holidays)\n",
    "- But too many might indicate a data problem\n",
    "- Need to know sample size for analysis\n",
    "- Some time series commands handle missings automatically, others don't\n",
    "\n",
    "---\n",
    "\n",
    "**The `summarize` Command**\n",
    "\n",
    "**Basic Syntax:** `summarize [varlist] [if] [in] [, detail]`\n",
    "\n",
    "**Without `detail` option (default):**\n",
    "- Shows: N, mean, std. dev., min, max\n",
    "- Fast summary for quick checks\n",
    "\n",
    "**With `detail` option:**\n",
    "```stata\n",
    "summarize sp500_close, detail\n",
    "```\n",
    "\n",
    "**Provides comprehensive statistics:**\n",
    "- **Percentiles:** 1%, 5%, 10%, 25%, 50%, 75%, 90%, 95%, 99%\n",
    "- **Moments:** \n",
    "  - Mean and standard deviation\n",
    "  - Variance\n",
    "  - Skewness: measure of asymmetry (0 for symmetric)\n",
    "  - Kurtosis: measure of tail thickness (3 for normal distribution)\n",
    "- **Extremes:** Smallest and largest values\n",
    "- **Sum of weights** and **sum of variable**\n",
    "\n",
    "**What to look for in S&P 500 data:**\n",
    "- **Min/Max:** Should be positive (it's a price level)\n",
    "- **Mean vs Median:** If very different, distribution is skewed\n",
    "- **Skewness:** Stock prices often right-skewed (occasional large increases)\n",
    "- **Outliers:** Check if smallest/largest values make sense\n",
    "- **Sample period:** Inferred from number of observations (roughly 252 trading days/year)\n",
    "\n",
    "**Why `detail` is important for time series:**\n",
    "- Helps identify outliers before they affect model estimation\n",
    "- Distributional properties matter for model assumptions\n",
    "- Can reveal data entry errors (e.g., negative prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e515bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  95\n",
      "\n",
      "\n",
      "                            SP500\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%      1923.67        1829.08\n",
      " 5%      2081.72        1851.86\n",
      "10%       2181.9        1852.21       Obs               2,514\n",
      "25%      2675.81        1853.44       Sum of wgt.       2,514\n",
      "\n",
      "50%     3509.945                      Mean           3695.084\n",
      "                        Largest       Std. dev.       1251.14\n",
      "75%         4470        6851.97\n",
      "90%      5650.38        6875.16       Variance        1565350\n",
      "95%      6037.88        6890.59       Skewness       .5599163\n",
      "99%      6654.72        6890.89       Kurtosis       2.345057\n"
     ]
    }
   ],
   "source": [
    "* Rename price variable (column name matches filename: SP500)\n",
    "rename sp500 sp500_close\n",
    "\n",
    "* Check for missing values\n",
    "count if missing(sp500_close)\n",
    "summarize sp500_close, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c2814a",
   "metadata": {},
   "source": [
    "### Step 4: Declaring Time Series Structure with `tsset`\n",
    "\n",
    "This is one of the **most important commands** in Stata time series analysis. Without `tsset`, you cannot use time series operators.\n",
    "\n",
    "**The `tsset` Command**\n",
    "\n",
    "**Syntax:** \n",
    "```stata\n",
    "tsset timevar [, options]              // for single time series\n",
    "tsset panelvar timevar [, options]     // for panel data\n",
    "```\n",
    "\n",
    "**What `tsset` does:**\n",
    "1. **Declares** which variable contains the time index\n",
    "2. **Validates** the time series structure:\n",
    "   - Checks for duplicates (each time point should appear once)\n",
    "   - Verifies chronological ordering\n",
    "   - Identifies gaps (missing time periods)\n",
    "3. **Enables** time series operators: `L.`, `F.`, `D.`, `S.`\n",
    "4. **Stores** time series settings for the dataset\n",
    "\n",
    "**Our command:**\n",
    "```stata\n",
    "tsset date\n",
    "```\n",
    "\n",
    "**Common options:**\n",
    "- `delta(#)`: Specify time between observations (for irregular data)\n",
    "- `format(%fmt)`: Override default time format\n",
    "- `yearly|quarterly|monthly|weekly|daily`: Specify frequency\n",
    "\n",
    "**What Stata checks:**\n",
    "- ✓ No duplicate time values\n",
    "- ✓ Data is sorted by time\n",
    "- ✓ Identifies gaps (e.g., weekends in daily financial data)\n",
    "\n",
    "**Output information:**\n",
    "- Time variable name and type\n",
    "- Delta: time between observations\n",
    "- Range: first to last time point  \n",
    "- Number of periods\n",
    "- Number of gaps (expected for financial data)\n",
    "\n",
    "---\n",
    "\n",
    "**Why `tsset` is Required**\n",
    "\n",
    "**Time Series Operators Only Work After `tsset`:**\n",
    "\n",
    "These operators reference other time periods relative to current observation:\n",
    "\n",
    "1. **Lag operator `L.`**: Previous period\n",
    "   - `L.varname` = value from t-1\n",
    "   - `L2.varname` = value from t-2\n",
    "   - `L3.varname` = value from t-3, etc.\n",
    "\n",
    "2. **Lead operator `F.`**: Next period\n",
    "   - `F.varname` = value from t+1\n",
    "   - `F2.varname` = value from t+2\n",
    "\n",
    "3. **Difference operator `D.`**: Change from previous period\n",
    "   - `D.varname` = `varname - L.varname`\n",
    "   - `D2.varname` = second difference = `D.(D.varname)`\n",
    "\n",
    "4. **Seasonal difference `S.`**: Change from same season previous year\n",
    "   - `S12.varname` for monthly data (12 months ago)\n",
    "   - `S4.varname` for quarterly data (4 quarters ago)\n",
    "\n",
    "**Without `tsset`, these operators won't work!** Stata won't know what \"previous period\" means.\n",
    "\n",
    "---\n",
    "\n",
    "**Gap Handling**\n",
    "\n",
    "**Financial data typically has gaps:**\n",
    "- Stock markets: closed weekends and holidays\n",
    "- Daily data: 252 trading days/year, not 365\n",
    "- Stata identifies these automatically\n",
    "- Gaps don't break time series operators (Stata skips them correctly)\n",
    "\n",
    "**Example:** If Friday is t=5 and Monday is t=8:\n",
    "- On Monday: `L.price` correctly references Friday (not Sunday)\n",
    "- Stata uses the date values, not observation numbers\n",
    "\n",
    "---\n",
    "\n",
    "**Panel Data Extension**\n",
    "\n",
    "For panel data (multiple entities over time):\n",
    "```stata\n",
    "tsset country_id year\n",
    "```\n",
    "- First variable: cross-section identifier (country, firm, individual)\n",
    "- Second variable: time identifier\n",
    "- Allows time series operators within each panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0377ff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time variable: date, 04nov2015 to 03nov2025, but with gaps\n",
      "        Delta: 1 day\n",
      "\n",
      "(95 missing values generated)\n",
      "\n",
      "(643 missing values generated)\n",
      "\n",
      "\n",
      "\n",
      "                        sp500_return\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.0342684      -.0999451\n",
      " 5%    -.0172119      -.0616093\n",
      "10%    -.0111666      -.0607519       Obs               1,966\n",
      "25%     -.003767      -.0532222       Sum of wgt.       1,966\n",
      "\n",
      "50%     .0006955                      Mean           .0005213\n",
      "                        Largest       Std. dev.      .0112914\n",
      "75%     .0058107        .060544\n",
      "90%     .0117779       .0888085       Variance       .0001275\n",
      "95%     .0160189       .0896831       Skewness      -.0679821\n",
      "99%     .0258198       .0908947       Kurtosis       15.01661\n"
     ]
    }
   ],
   "source": [
    "* Set time series (required before using D. operator)\n",
    "tsset date\n",
    "\n",
    "* Create log price and log returns\n",
    "* Drop if already exists (in case cell is re-run)\n",
    "gen log_sp500 = ln(sp500_close)\n",
    "gen sp500_return = D.log_sp500\n",
    "\n",
    "* Drop log price (keep only close and return)\n",
    "drop log_sp500\n",
    "\n",
    "* Check returns summary\n",
    "summarize sp500_return, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec047e1b",
   "metadata": {},
   "source": [
    "### Step 5: Computing Log Returns - Theory and Practice\n",
    "\n",
    "Returns are the fundamental building block of financial time series analysis. We need to transform prices into returns for several theoretical and practical reasons.\n",
    "\n",
    "---\n",
    "\n",
    "**Why Returns Instead of Prices?**\n",
    "\n",
    "**Theoretical reasons:**\n",
    "\n",
    "1. **Stationarity**: Prices typically have trends (non-stationary), returns are closer to stationary\n",
    "2. **Scale-free**: Returns are comparable across assets regardless of price level\n",
    "3. **Interpretability**: Returns represent investor gains/losses\n",
    "\n",
    "**Mathematical definition:**\n",
    "\n",
    "**Simple returns:**\n",
    "$$R_t = \\frac{P_t - P_{t-1}}{P_{t-1}} = \\frac{P_t}{P_{t-1}} - 1$$\n",
    "\n",
    "**Log returns (continuously compounded):**\n",
    "$$r_t = \\ln(P_t) - \\ln(P_{t-1}) = \\ln\\left(\\frac{P_t}{P_{t-1}}\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "**Why Log Returns?**\n",
    "\n",
    "**Advantages of log returns over simple returns:**\n",
    "\n",
    "1. **Time-additivity**: Multi-period returns sum nicely\n",
    "   $$r_{t,t+k} = \\sum_{i=1}^{k} r_{t+i}$$\n",
    "   \n",
    "   For simple returns, you must compound: $(1+R_1)(1+R_2) - 1$\n",
    "\n",
    "2. **Symmetry**: \n",
    "   - +50% followed by -50% (simple) gives: $1.5 \\times 0.5 = 0.75$ (net loss!)\n",
    "   - For small changes, log returns are symmetric: $\\ln(1.5) \\approx 0.405$, $\\ln(0.5) \\approx -0.693$\n",
    "\n",
    "3. **Approximate percentage change**: For small $r$, $r_t \\approx R_t$\n",
    "   $$\\ln(1 + R_t) \\approx R_t \\text{ when } |R_t| \\text{ is small}$$\n",
    "\n",
    "4. **Statistical properties**:\n",
    "   - More likely to be normally distributed\n",
    "   - Variance tends to be more stable over time\n",
    "   - Better suited for econometric models (ARMA, GARCH)\n",
    "\n",
    "5. **Cross-sectional properties**: If $r_A$ and $r_B$ are log returns, the portfolio return is approximately the weighted sum (for equal weights: $(r_A + r_B)/2$)\n",
    "\n",
    "---\n",
    "\n",
    "**Stata Implementation**\n",
    "\n",
    "**The `ln()` function:**\n",
    "- Computes natural logarithm (base $e$)\n",
    "- Alternative: `log()` (same as `ln()`)\n",
    "- For base 10: `log10()`\n",
    "\n",
    "**Step-by-step process:**\n",
    "```stata\n",
    "gen log_sp500 = ln(sp500_close)    // Take log of price\n",
    "gen sp500_return = D.log_sp500      // First difference of log\n",
    "```\n",
    "\n",
    "**What `D.` does:**\n",
    "- `D.varname` = `varname - L.varname`\n",
    "- For log_sp500: $\\ln(P_t) - \\ln(P_{t-1})$ = log return\n",
    "- Automatically handles time series structure (requires `tsset` first)\n",
    "- First observation becomes missing (no previous value)\n",
    "\n",
    "**Alternative without `D.` operator:**\n",
    "```stata\n",
    "gen sp500_return = ln(sp500_close) - ln(L.sp500_close)\n",
    "```\n",
    "This is equivalent but less concise.\n",
    "\n",
    "**Or in one step:**\n",
    "```stata\n",
    "gen sp500_return = D.ln(sp500_close)\n",
    "```\n",
    "Both approaches work; we use two steps for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "**Why We Drop the Intermediate `log_sp500`:**\n",
    "\n",
    "We created `log_sp500` only to compute returns. For most financial analysis:\n",
    "- We care about **returns** (changes), not log levels\n",
    "- Keeping unnecessary variables clutters the dataset\n",
    "- If needed later, we can always recreate it from prices\n",
    "\n",
    "**When to keep log levels:**\n",
    "- Cointegration analysis (testing relationships between log price levels)\n",
    "- Error correction models\n",
    "- Long-run equilibrium analysis\n",
    "\n",
    "---\n",
    "\n",
    "**Checking the Results**\n",
    "\n",
    "**The `summarize sp500_return, detail` command shows:**\n",
    "\n",
    "**Expected properties of daily stock returns:**\n",
    "- **Mean ≈ 0**: Stock returns have small average daily gain (maybe 0.03% to 0.05%)\n",
    "- **Std Dev ≈ 1-2%**: Daily volatility typically 1-2% for broad indices\n",
    "- **Skewness < 0**: Returns often left-skewed (large drops more common than large gains)\n",
    "- **Kurtosis > 3**: \"Fat tails\" - extreme events more common than normal distribution\n",
    "- **Min/Max**: Check for outliers (crashes: < -10%, rallies: > 10%)\n",
    "\n",
    "**Common issues to check:**\n",
    "- Returns > 100%: Likely data error (unless stock split not adjusted)\n",
    "- Too many zeros: Possible stale prices or data issues  \n",
    "- First observation missing: Normal (no previous price to compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2e896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time variable: date, 04nov2015 to 03nov2025, but with gaps\n",
      "        Delta: 1 day\n",
      "\n",
      "\n",
      "Time variable: date, 04nov2015 to 03nov2025, but with gaps\n",
      "        Delta: 1 day\n"
     ]
    }
   ],
   "source": [
    "* Add variable labels\n",
    "label var date \"Date\"\n",
    "label var sp500_close \"S&P 500 Closing Price\"\n",
    "label var sp500_return \"S&P 500 Log Returns\"\n",
    "\n",
    "* Set time series\n",
    "tsset date\n",
    "\n",
    "* Check time series structure\n",
    "tsset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd0e54",
   "metadata": {},
   "source": [
    "### Step 6: Adding Variable Labels for Documentation\n",
    "\n",
    "Proper documentation is crucial for reproducibility and collaboration. Variable labels serve as built-in documentation.\n",
    "\n",
    "**The `label var` Command**\n",
    "\n",
    "**Syntax:** `label variable varname \"label text\"`  \n",
    "**Short form:** `label var varname \"label text\"`\n",
    "\n",
    "**What it does:**\n",
    "- Attaches a descriptive text label to a variable\n",
    "- Appears in output from `describe`, `summarize`, graphs, tables\n",
    "- Stored with the dataset when you `save`\n",
    "- Maximum 80 characters\n",
    "\n",
    "**Our commands:**\n",
    "```stata\n",
    "label var date \"Date\"\n",
    "label var sp500_close \"S&P 500 Closing Price\"\n",
    "label var sp500_return \"S&P 500 Log Returns\"\n",
    "```\n",
    "\n",
    "**Best practices:**\n",
    "- **Be descriptive but concise**: \"S&P 500 Closing Price\" not just \"Price\"\n",
    "- **Include units if relevant**: \"GDP in billions of 2012 USD\"\n",
    "- **Specify transformations**: \"Log Returns\" not just \"Returns\"\n",
    "- **Update labels if you transform**: If you create `log_gdp`, label it as such\n",
    "\n",
    "**Benefits:**\n",
    "- Future you will remember what variables mean\n",
    "- Graphs automatically use labels for axes titles\n",
    "- Regression output shows labels instead of cryptic variable names\n",
    "- Collaborators can understand your data\n",
    "\n",
    "---\n",
    "\n",
    "**Checking Time Series Properties with `tsset`**\n",
    "\n",
    "After setting up the time series structure earlier with `tsset date`, we can run `tsset` again without arguments to **check** the current settings:\n",
    "\n",
    "**What `tsset` (no arguments) displays:**\n",
    "```\n",
    "Time variable: date, 01jan1990 to 31dec2023\n",
    "        Delta: 1 day\n",
    "```\n",
    "\n",
    "**Information shown:**\n",
    "- **Time variable**: Confirms which variable is the time index\n",
    "- **Range**: First and last observation dates\n",
    "- **Delta**: Time between consecutive observations (1 day for daily data)\n",
    "- **Gaps**: Number of gaps (weekends/holidays for financial data)\n",
    "\n",
    "**Why check this:**\n",
    "- Confirms `tsset` worked correctly\n",
    "- Verifies the time range (sample period)\n",
    "- Identifies gaps (expected ~104 weekend days + ~10 holidays per year)\n",
    "\n",
    "**Troubleshooting:**\n",
    "- If you get \"data not tsset\": you need to run `tsset timevar`\n",
    "- If you get \"repeated time values\": you have duplicates (use `duplicates drop`)\n",
    "- If Delta is wrong: might have wrong time frequency or irregular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aee0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file data/processed/sp500_data.dta saved\n",
      "\n",
      "S&P 500 data processed and saved successfully!\n",
      "\n",
      "\n",
      "Contains data from data/processed/sp500_data.dta\n",
      " Observations:         2,609                  \n",
      "    Variables:             3                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "sp500_close     float   %9.0g                 S&P 500 Closing Price\n",
      "date            float   %td                   Date\n",
      "sp500_return    float   %9.0g                 S&P 500 Log Returns\n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n"
     ]
    }
   ],
   "source": [
    "* Save processed S&P 500 data\n",
    "save \"data/processed/sp500_data.dta\", replace\n",
    "\n",
    "display \"S&P 500 data processed and saved successfully!\"\n",
    "describe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cfe54",
   "metadata": {},
   "source": [
    "### Step 7: Saving the Processed Dataset\n",
    "\n",
    "Now that we've cleaned and transformed the data, we need to save it for use in subsequent analyses.\n",
    "\n",
    "**The `save` Command**\n",
    "\n",
    "**Syntax:** `save filename [, options]`\n",
    "\n",
    "**Key options:**\n",
    "- `replace`: Overwrite file if it already exists (without this, Stata throws an error)\n",
    "- `orphans`: Save even if value labels aren't attached to variables (rare use)\n",
    "- `all`: Include all characteristics and stored results (advanced)\n",
    "\n",
    "**Our command:**\n",
    "```stata\n",
    "save \"data/processed/sp500_data.dta\", replace\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- Saves current data in memory to a Stata `.dta` file\n",
    "- Preserves:\n",
    "  - All variables and observations\n",
    "  - Variable labels\n",
    "  - Variable formats\n",
    "  - tsset settings (time series structure)\n",
    "  - Sort order\n",
    "  - Value labels (if any)\n",
    "  - Data characteristics\n",
    "- Location: `data/processed/` (separate from raw data)\n",
    "\n",
    "---\n",
    "\n",
    "**File Organization Best Practices**\n",
    "\n",
    "**The raw vs. processed distinction:**\n",
    "\n",
    "**Raw data** (`data/raw/`):\n",
    "- Original, unmodified files from source (FRED, Bloomberg, etc.)\n",
    "- **Never modify these files!**\n",
    "- Keep them as a permanent record\n",
    "- Can always regenerate processed data from raw\n",
    "\n",
    "**Processed data** (`data/processed/`):\n",
    "- Cleaned, transformed, ready-to-analyze versions\n",
    "- Created by reproducible scripts/notebooks\n",
    "- Can be deleted and regenerated\n",
    "- Specific to your analysis needs\n",
    "\n",
    "**Why this separation matters:**\n",
    "- **Reproducibility**: Always know where data came from\n",
    "- **Version control**: Can track changes to processing code\n",
    "- **Collaboration**: Clear which files are source vs. derived\n",
    "- **Data integrity**: Raw data never gets corrupted by analysis\n",
    "\n",
    "---\n",
    "\n",
    "**The `.dta` Format**\n",
    "\n",
    "Stata's native binary format:\n",
    "- **Efficient**: Compressed, fast to load\n",
    "- **Comprehensive**: Stores metadata (labels, formats, etc.)\n",
    "- **Portable**: Works across operating systems\n",
    "- **Version-aware**: Stata handles different file versions\n",
    "\n",
    "**Alternatives for sharing:**\n",
    "- `.csv`: Universal but loses metadata (labels, formats)\n",
    "- `.xlsx`: Good for viewing but inefficient for large datasets\n",
    "- `.dta`: Best for Stata-to-Stata workflow\n",
    "\n",
    "**Loading saved data:**\n",
    "```stata\n",
    "use \"data/processed/sp500_data.dta\", clear\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**The `display` Command**\n",
    "\n",
    "**Syntax:** `display [display_directive] [display_directive ...]`\n",
    "\n",
    "**Simple use:**\n",
    "```stata\n",
    "display \"S&P 500 data processed and saved successfully!\"\n",
    "```\n",
    "\n",
    "**What it does:**\n",
    "- Prints text to the Results window\n",
    "- Useful for status messages in scripts\n",
    "- Can also compute and display expressions\n",
    "\n",
    "**More advanced uses:**\n",
    "```stata\n",
    "display \"Number of observations: \" _N\n",
    "display \"Mean return: \" %6.4f r(mean)\n",
    "display \"Today's date: \" %tdDDmonYY date(\"$S_DATE\", \"DMY\")\n",
    "```\n",
    "\n",
    "**Display formats:**\n",
    "- `%6.4f`: Fixed format (6 total width, 4 decimals)\n",
    "- `%9.0g`: General format\n",
    "- `%td`: Date format\n",
    "- `_N`: System variable for number of observations\n",
    "\n",
    "**Why use `display` in notebooks:**\n",
    "- Provides feedback on workflow progress\n",
    "- Confirms steps completed successfully\n",
    "- Useful for debugging (print variable values)\n",
    "\n",
    "---\n",
    "\n",
    "**Final Verification**\n",
    "\n",
    "The `describe` command at the end confirms:\n",
    "- Number of observations and variables\n",
    "- Variable names and types\n",
    "- Storage format\n",
    "- Variable labels we added\n",
    "- Dataset is ready for analysis\n",
    "\n",
    "**What to check:**\n",
    "- ✓ Expected number of observations (roughly 252 trading days/year × years)\n",
    "- ✓ Two variables: `date` and returns (we dropped log_sp500)\n",
    "- ✓ Labels are present and correct\n",
    "- ✓ File saved in correct location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991d3d4",
   "metadata": {},
   "source": [
    "<style>\n",
    "h2.styled-header {\n",
    "    max-width: 600px;\n",
    "    margin: 20px auto 22px !important;\n",
    "    padding: 20px 32px !important;\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid #e5e7eb;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%);\n",
    "    box-shadow: 0 8px 26px rgba(0,0,0,0.06);\n",
    "    overflow: hidden;\n",
    "    text-align: center;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 800 !important;\n",
    "    color: #14276c !important;\n",
    "    margin-bottom: 8px !important;\n",
    "    margin-top: 0 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2 class=\"styled-header\">4) EUR/USD Exchange Rate Data</h2>\n",
    "\n",
    "Process daily EUR/USD exchange rate data. This will be used for unit root testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29d69a",
   "metadata": {},
   "source": [
    "## Processing Strategy for EUR/USD Data\n",
    "\n",
    "The EUR/USD exchange rate data follows the same general workflow as the S&P 500 data we just processed:\n",
    "1. Import CSV\n",
    "2. Parse dates  \n",
    "3. Check for duplicates and missing values\n",
    "4. Transform the data\n",
    "5. Add labels\n",
    "6. Save\n",
    "\n",
    "**Key Difference:** For exchange rates, we'll create **multiple transformations** and keep them all:\n",
    "- **Original level** (`eurusd`): For level analysis, cointegration tests\n",
    "- **Log level** (`lneurusd`): For unit root tests in logs\n",
    "- **First difference** (`d_lneurusd`): For testing stationarity, returns analysis\n",
    "\n",
    "This is because unit root testing (which we'll do in the next notebook) often requires examining the series in multiple forms.\n",
    "\n",
    "**Why EUR/USD?**\n",
    "- Major global exchange rate (most liquid currency pair)\n",
    "- Benchmark for foreign exchange analysis\n",
    "- Classic example for unit root tests and random walk hypothesis\n",
    "- Relevant for international finance and macroeconomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(encoding automatically selected: ISO-8859-1)\n",
      "(2 vars, 7,000 obs)\n",
      "\n",
      "\n",
      "     +----------------------+\n",
      "     | observat~e   dexuseu |\n",
      "     |----------------------|\n",
      "  1. | 1999-01-04    1.1812 |\n",
      "  2. | 1999-01-05     1.176 |\n",
      "  3. | 1999-01-06    1.1636 |\n",
      "  4. | 1999-01-07    1.1672 |\n",
      "  5. | 1999-01-08    1.1554 |\n",
      "     |----------------------|\n",
      "  6. | 1999-01-11    1.1534 |\n",
      "  7. | 1999-01-12    1.1548 |\n",
      "  8. | 1999-01-13    1.1698 |\n",
      "  9. | 1999-01-14    1.1689 |\n",
      " 10. | 1999-01-15    1.1591 |\n",
      "     +----------------------+\n",
      "\n",
      "\n",
      "Contains data\n",
      " Observations:         7,000                  \n",
      "    Variables:             2                  \n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "observation_d~e str10   %10s                  \n",
      "dexuseu         float   %9.0g                 DEXUSEU\n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: \n",
      "     Note: Dataset has changed since last saved.\n"
     ]
    }
   ],
   "source": [
    "* Load EUR/USD raw data\n",
    "import delimited \"data/raw/DEXUSEU.csv\", clear\n",
    "\n",
    "* Display first few observations\n",
    "list in 1/10\n",
    "\n",
    "* Check data structure\n",
    "describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81856d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "--------------------------------------\n",
      "   Copies | Observations       Surplus\n",
      "----------+---------------------------\n",
      "        1 |         7000             0\n",
      "--------------------------------------\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "(0 observations are duplicates)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "* Parse dates from observation_date column\n",
    "gen date_numeric = date(observation_date, \"YMD\")\n",
    "drop observation_date\n",
    "\n",
    "* Format as Stata daily date\n",
    "format date_numeric %td\n",
    "rename date_numeric date\n",
    "\n",
    "* Check for duplicates\n",
    "duplicates report date\n",
    "duplicates drop date, force\n",
    "\n",
    "* Sort by date\n",
    "sort date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb7f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  270\n",
      "\n",
      "\n",
      "                           DEXUSEU\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%        .8598           .827\n",
      " 5%        .9037          .8271\n",
      "10%       .98485          .8364       Obs               6,730\n",
      "25%       1.0844          .8364       Sum of wgt.       6,730\n",
      "\n",
      "50%       1.1702                      Mean           1.182768\n",
      "                        Largest       Std. dev.      .1540254\n",
      "75%       1.2994         1.5923\n",
      "90%      1.37945         1.5924       Variance       .0237238\n",
      "95%       1.4399         1.5978       Skewness       .1009166\n",
      "99%       1.5569          1.601       Kurtosis       2.676944\n"
     ]
    }
   ],
   "source": [
    "* Rename exchange rate variable (column name matches filename: DEXUSEU)\n",
    "rename dexuseu eurusd\n",
    "\n",
    "* Check for missing values\n",
    "count if missing(eurusd)\n",
    "summarize eurusd, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bcc868",
   "metadata": {},
   "source": [
    "### Understanding Exchange Rate Data\n",
    "\n",
    "**What is DEXUSEU?**\n",
    "- FRED ticker for U.S. Dollars to One Euro exchange rate\n",
    "- **Interpretation**: How many USD needed to buy 1 EUR\n",
    "  - If DEXUSEU = 1.10, then 1 EUR = 1.10 USD\n",
    "  - Increase means EUR appreciation (USD depreciation)\n",
    "  - Decrease means EUR depreciation (USD appreciation)\n",
    "\n",
    "**Missing Values in Exchange Rate Data:**\n",
    "\n",
    "Exchange rate data has missing values for the same reason as stock prices:\n",
    "- Weekends: No trading Saturday/Sunday\n",
    "- Holidays: Both U.S. and European holidays\n",
    "- Market closures: Occasional disruptions\n",
    "\n",
    "The `count if missing()` command helps us quantify this. We expect:\n",
    "- ~104 weekend days per year\n",
    "- ~15-20 holidays per year (varies by year and whether holidays fall on weekdays)\n",
    "- Total: ~30-35% of calendar days missing (roughly 130 missing days per year)\n",
    "\n",
    "This is **normal and expected** for financial data. Time series operators in Stata handle gaps correctly after `tsset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12318094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time variable: date, 04jan1999 to 31oct2025, but with gaps\n",
      "        Delta: 1 day\n",
      "\n",
      "(270 missing values generated)\n",
      "\n",
      "(1,742 missing values generated)\n",
      "\n",
      "\n",
      "                           DEXUSEU\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%        .8598           .827\n",
      " 5%        .9037          .8271\n",
      "10%       .98485          .8364       Obs               6,730\n",
      "25%       1.0844          .8364       Sum of wgt.       6,730\n",
      "\n",
      "50%       1.1702                      Mean           1.182768\n",
      "                        Largest       Std. dev.      .1540254\n",
      "75%       1.2994         1.5923\n",
      "90%      1.37945         1.5924       Variance       .0237238\n",
      "95%       1.4399         1.5978       Skewness       .1009166\n",
      "99%       1.5569          1.601       Kurtosis       2.676944\n",
      "\n",
      "                          lneurusd\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.1510555      -.1899506\n",
      " 5%    -.1012578      -.1898297\n",
      "10%    -.0152659      -.1786483       Obs               6,730\n",
      "25%     .0810269      -.1786483       Sum of wgt.       6,730\n",
      "\n",
      "50%     .1571747                      Mean           .1592579\n",
      "                        Largest       Std. dev.      .1318484\n",
      "75%     .2619026       .4651796\n",
      "90%     .3216849       .4652423       Variance        .017384\n",
      "95%     .3645737       .4686277       Skewness      -.2322385\n",
      "99%     .4426967       .4706284       Kurtosis       2.770172\n",
      "\n",
      "                         d_lneurusd\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     -.015197       -.030031\n",
      " 5%    -.0095197      -.0267241\n",
      "10%    -.0067176      -.0219574       Obs               5,258\n",
      "25%    -.0031692      -.0219432       Sum of wgt.       5,258\n",
      "\n",
      "50%     -.000073                      Mean           .0000154\n",
      "                        Largest       Std. dev.      .0058038\n",
      "75%     .0031839       .0296149\n",
      "90%     .0069673       .0306427       Variance       .0000337\n",
      "95%     .0094788       .0389142       Skewness       .1440726\n",
      "99%     .0145946        .046208       Kurtosis       5.551395\n"
     ]
    }
   ],
   "source": [
    "* Set time series (required before using D. operator)\n",
    "tsset date\n",
    "\n",
    "* Create log exchange rate and first difference\n",
    "gen lneurusd = ln(eurusd)\n",
    "gen d_lneurusd = D.lneurusd\n",
    "\n",
    "* Check summary statistics\n",
    "summarize eurusd lneurusd d_lneurusd, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513ba17",
   "metadata": {},
   "source": [
    "### Exchange Rate Transformations: Theory and Application\n",
    "\n",
    "For exchange rates, we create two transformations that are essential for time series econometrics.\n",
    "\n",
    "---\n",
    "\n",
    "**Transformation 1: Log Exchange Rate**\n",
    "\n",
    "**Theory: Why take logs of exchange rates?**\n",
    "\n",
    "The log transformation has several important properties for exchange rates:\n",
    "\n",
    "1. **Symmetry property:**\n",
    "   - If EUR/USD goes from 1.00 to 1.10 (+10%), the reverse rate USD/EUR goes from 1.00 to 0.909 (-9.1%)\n",
    "   - In logs: $\\ln(1.10) = 0.095$ and $\\ln(0.909) = -0.095$ (symmetric!)\n",
    "   - This makes appreciation and depreciation symmetric\n",
    "\n",
    "2. **Interpretability:**\n",
    "   - Small changes in log exchange rate ≈ percentage changes\n",
    "   - $\\Delta \\ln(S_t) \\approx \\frac{S_t - S_{t-1}}{S_{t-1}}$ for small changes\n",
    "\n",
    "3. **Statistical properties:**\n",
    "   - Log exchange rates often more normally distributed than levels\n",
    "   - Variance stabilization (reduces heteroskedasticity)\n",
    "\n",
    "4. **Theoretical models:**\n",
    "   - Many exchange rate theories (PPP, UIP, monetary models) expressed in logs\n",
    "   - Facilitates elasticity interpretation in regressions\n",
    "\n",
    "**Stata implementation:**\n",
    "```stata\n",
    "gen lneurusd = ln(eurusd)\n",
    "```\n",
    "\n",
    "The `ln()` function computes the natural logarithm (base $e$).\n",
    "\n",
    "---\n",
    "\n",
    "**Transformation 2: First Difference of Log Exchange Rate**\n",
    "\n",
    "**Theory: Differencing for stationarity**\n",
    "\n",
    "**Definition:** The first difference operator:\n",
    "$$\\Delta x_t = x_t - x_{t-1}$$\n",
    "\n",
    "Applied to log exchange rate:\n",
    "$$\\Delta \\ln(S_t) = \\ln(S_t) - \\ln(S_{t-1}) = \\ln\\left(\\frac{S_t}{S_{t-1}}\\right)$$\n",
    "\n",
    "This is the **continuously compounded return** or **growth rate** of the exchange rate.\n",
    "\n",
    "**Why difference?**\n",
    "\n",
    "**Stationarity concept:**\n",
    "- A time series is **stationary** if its statistical properties (mean, variance, autocorrelations) don't change over time\n",
    "- **Non-stationary** series often have trends, changing variance, or unit roots\n",
    "- Most econometric methods require stationary data\n",
    "\n",
    "**Integration and unit roots:**\n",
    "- A series with a **unit root** is integrated of order 1, denoted $I(1)$\n",
    "- If $x_t \\sim I(1)$, then $\\Delta x_t \\sim I(0)$ (stationary)\n",
    "- Exchange rates are typically $I(1)$: the level has a unit root, but the change is stationary\n",
    "\n",
    "**Random walk hypothesis:**\n",
    "$$S_t = S_{t-1} + \\varepsilon_t$$\n",
    "\n",
    "where $\\varepsilon_t$ is white noise. This implies:\n",
    "$$\\Delta S_t = \\varepsilon_t \\sim I(0)$$\n",
    "\n",
    "For log exchange rate:\n",
    "$$\\ln(S_t) = \\ln(S_{t-1}) + u_t$$\n",
    "$$\\Delta \\ln(S_t) = u_t$$\n",
    "\n",
    "The random walk hypothesis suggests exchange rate changes are unpredictable (efficient markets).\n",
    "\n",
    "**Stata implementation:**\n",
    "```stata\n",
    "gen d_lneurusd = D.lneurusd\n",
    "```\n",
    "\n",
    "The `D.` operator:\n",
    "- Requires `tsset` first\n",
    "- Computes `varname - L.varname`\n",
    "- Automatically makes first observation missing (no previous value)\n",
    "- Handles gaps correctly (weekends, holidays)\n",
    "\n",
    "**Alternative (equivalent):**\n",
    "```stata\n",
    "gen d_lneurusd = lneurusd - L.lneurusd\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Why Keep Both `lneurusd` and `d_lneurusd`?**\n",
    "\n",
    "Different tests and models require different transformations:\n",
    "\n",
    "1. **Unit root tests** (Dickey-Fuller, Phillips-Perron):\n",
    "   - Test whether level has unit root\n",
    "   - Test whether first difference is stationary\n",
    "   - Need both to determine order of integration\n",
    "\n",
    "2. **Cointegration analysis:**\n",
    "   - Tests long-run relationships between levels\n",
    "   - Requires non-stationary levels\n",
    "\n",
    "3. **ARMA/VAR models:**\n",
    "   - Typically use stationary (differenced) data\n",
    "   - But need levels to interpret long-run effects\n",
    "\n",
    "4. **Forecasting:**\n",
    "   - Forecast differences, then cumulate to get level forecasts\n",
    "\n",
    "---\n",
    "\n",
    "**Interpreting the Summary Statistics**\n",
    "\n",
    "When we run `summarize eurusd lneurusd d_lneurusd, detail`, look for:\n",
    "\n",
    "**For `eurusd` (level):**\n",
    "- Range over sample period (e.g., 0.85 to 1.60 from 2000-2023)\n",
    "- High persistence (if we compute autocorrelations)\n",
    "\n",
    "**For `lneurusd` (log level):**\n",
    "- Similar but in log scale\n",
    "- Mean ≈ 0 to 0.4 (since ln(1.0) = 0, ln(1.5) ≈ 0.4)\n",
    "\n",
    "**For `d_lneurusd` (first difference / returns):**\n",
    "- **Mean ≈ 0**: Exchange rate has no persistent drift (on average)\n",
    "- **Std dev ≈ 0.5-1%**: Daily volatility of exchange rate\n",
    "- **Skewness**: Should be close to 0 (symmetric shocks)\n",
    "- **Kurtosis > 3**: Fat tails (occasional large jumps)\n",
    "- **Min/Max**: Look for extreme events (financial crises, interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e2f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time variable: date, 04jan1999 to 31oct2025, but with gaps\n",
      "        Delta: 1 day\n",
      "\n",
      "\n",
      "Time variable: date, 04jan1999 to 31oct2025, but with gaps\n",
      "        Delta: 1 day\n"
     ]
    }
   ],
   "source": [
    "* Add variable labels\n",
    "label var date \"Date\"\n",
    "label var eurusd \"EUR/USD Exchange Rate\"\n",
    "label var lneurusd \"Log EUR/USD Exchange Rate\"\n",
    "label var d_lneurusd \"First Difference of Log EUR/USD\"\n",
    "\n",
    "* Set time series\n",
    "tsset date\n",
    "\n",
    "* Check time series structure\n",
    "tsset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file data/processed/eurusd_data.dta saved\n",
      "\n",
      "EUR/USD data processed and saved successfully!\n",
      "\n",
      "\n",
      "Contains data from data/processed/eurusd_data.dta\n",
      " Observations:         7,000                  \n",
      "    Variables:             4                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "eurusd          float   %9.0g                 EUR/USD Exchange Rate\n",
      "date            float   %td                   Date\n",
      "lneurusd        float   %9.0g                 Log EUR/USD Exchange Rate\n",
      "d_lneurusd      float   %9.0g                 First Difference of Log EUR/USD\n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n"
     ]
    }
   ],
   "source": [
    "* Save processed EUR/USD data\n",
    "save \"data/processed/eurusd_data.dta\", replace\n",
    "\n",
    "display \"EUR/USD data processed and saved successfully!\"\n",
    "describe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af5133",
   "metadata": {},
   "source": [
    "<style>\n",
    "h2.styled-header {\n",
    "    max-width: 600px;\n",
    "    margin: 20px auto 22px !important;\n",
    "    padding: 20px 32px !important;\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid #e5e7eb;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%);\n",
    "    box-shadow: 0 8px 26px rgba(0,0,0,0.06);\n",
    "    overflow: hidden;\n",
    "    text-align: center;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 800 !important;\n",
    "    color: #14276c !important;\n",
    "    margin-bottom: 8px !important;\n",
    "    margin-top: 0 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2 class=\"styled-header\">5) Macro-Finance Data</h2>\n",
    "\n",
    "Process multiple macroeconomic and financial series and merge them into a single dataset for VAR analysis. We'll process each series separately, then merge on common dates.\n",
    "\n",
    "### 5.1 Real GDP (Quarterly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c94f10",
   "metadata": {},
   "source": [
    "## The Multi-Frequency Challenge\n",
    "\n",
    "Processing macroeconomic and financial data presents a unique challenge: **different variables are available at different frequencies**.\n",
    "\n",
    "**Our data frequencies:**\n",
    "- **Quarterly**: Real GDP (GDPC1)\n",
    "- **Monthly**: CPI, Interest Rate, Unemployment\n",
    "- **Daily**: Stock prices (S&P 500)\n",
    "\n",
    "**The fundamental problem:**\n",
    "- VAR (Vector Autoregression) models require all variables at the **same frequency**\n",
    "- We cannot directly combine quarterly and monthly data\n",
    "- We must choose a common frequency and convert all series\n",
    "\n",
    "**Our strategy:**\n",
    "1. **Convert daily → monthly**: Aggregate S&P 500 to monthly returns\n",
    "2. **Convert quarterly → monthly**: Interpolate or repeat GDP values\n",
    "3. **Keep monthly as is**: CPI, interest rates, unemployment\n",
    "4. **Merge all series**: Create unified monthly dataset\n",
    "\n",
    "**Alternative approaches:**\n",
    "\n",
    "1. **Mixed-frequency models** (MIDAS, MF-VAR):\n",
    "   - Advanced techniques that handle multiple frequencies\n",
    "   - More complex to implement and interpret\n",
    "   - Beyond scope of this course\n",
    "\n",
    "2. **Aggregate everything to quarterly:**\n",
    "   - Simpler, matches GDP frequency\n",
    "   - Loses information from higher-frequency data\n",
    "   - Only 4 observations per year (small sample)\n",
    "\n",
    "3. **Use only same-frequency variables:**\n",
    "   - Avoids conversion issues\n",
    "   - Limits scope of analysis (can't include GDP with monthly vars)\n",
    "\n",
    "**We choose monthly frequency because:**\n",
    "- Good balance: enough observations for VAR estimation (12/year)\n",
    "- Most macro variables available monthly\n",
    "- Standard practice in macro-financial VAR analysis\n",
    "- Facilitates policy analysis (monetary policy typically monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2cc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(encoding automatically selected: ISO-8859-1)\n",
      "(2 vars, 314 obs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time variable: date, 1947q1 to 2025q2\n",
      "        Delta: 1 quarter\n",
      "\n",
      "\n",
      "(1 missing value generated)\n",
      "\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "(0 observations are duplicates)\n",
      "\n",
      "\n",
      "(file data/processed/temp_gdp.dta not found)\n",
      "file data/processed/temp_gdp.dta saved\n",
      "\n",
      "\n",
      "Contains data from data/processed/temp_gdp.dta\n",
      " Observations:           314                  \n",
      "    Variables:             4                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "gdp             float   %9.0g                 GDPC1\n",
      "date            float   %tq                   \n",
      "lngdp           float   %9.0g                 \n",
      "gdp_growth      float   %9.0g                 \n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n",
      "\n",
      "\n",
      "                            GDPC1\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     2206.452       2172.432\n",
      " 5%     2559.214       2176.892\n",
      "10%     2936.852       2182.681       Obs                 314\n",
      "25%     4788.254       2206.452       Sum of wgt.         314\n",
      "\n",
      "50%     8768.826                      Mean           10236.19\n",
      "                        Largest       Std. dev.      6290.943\n",
      "75%     16136.73       23478.57\n",
      "90%     19506.95       23548.21       Variance       3.96e+07\n",
      "95%     21617.83       23586.54       Skewness       .4748699\n",
      "99%     23478.57       23770.98       Kurtosis       1.951788\n",
      "\n",
      "                            lngdp\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     7.699141       7.683602\n",
      " 5%     7.847456       7.685654\n",
      "10%     7.985094       7.688309       Obs                 314\n",
      "25%     8.473921       7.699141       Sum of wgt.         314\n",
      "\n",
      "50%     9.078956                      Mean           9.013419\n",
      "                        Largest       Std. dev.      .6992524\n",
      "75%     9.688853       10.06384\n",
      "90%     9.878526        10.0668       Variance        .488954\n",
      "95%     9.981274       10.06843       Skewness      -.2567308\n",
      "99%     10.06384       10.07622       Kurtosis       1.837324\n",
      "\n",
      "                         gdp_growth\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.0208197      -.0820427\n",
      " 5%    -.0091457      -.0263023\n",
      "10%    -.0034571      -.0221338       Obs                 313\n",
      "25%      .003087      -.0208197       Sum of wgt.         313\n",
      "\n",
      "50%     .0077009                      Mean           .0076291\n",
      "                        Largest       Std. dev.      .0111068\n",
      "75%     .0122204        .037919\n",
      "90%      .019022       .0379224       Variance       .0001234\n",
      "95%      .022439       .0385637       Skewness      -1.022701\n",
      "99%      .037919       .0747576       Kurtosis       20.08507\n"
     ]
    }
   ],
   "source": [
    "* Load Real GDP data (quarterly)\n",
    "import delimited \"data/raw/GDPC1.csv\", clear\n",
    "\n",
    "* Parse dates from observation_date column\n",
    "gen date_numeric = date(observation_date, \"YMD\")\n",
    "drop observation_date\n",
    "\n",
    "* Convert to quarterly format\n",
    "* First convert to daily, then to quarterly\n",
    "format date_numeric %td\n",
    "gen year = year(date_numeric)\n",
    "gen month = month(date_numeric)\n",
    "gen quarter = ceil(month/3)\n",
    "gen date_q = yq(year, quarter)\n",
    "format date_q %tq\n",
    "\n",
    "* Rename GDP variable (column name matches filename: GDPC1)\n",
    "rename gdpc1 gdp\n",
    "\n",
    "* Set time series (required before using D. operator)\n",
    "rename date_q date\n",
    "tsset date\n",
    "\n",
    "* Create log GDP and growth rate\n",
    "gen lngdp = ln(gdp)\n",
    "gen gdp_growth = D.lngdp\n",
    "\n",
    "* Keep only quarterly date and variables\n",
    "keep date gdp lngdp gdp_growth\n",
    "\n",
    "* Remove duplicates and sort\n",
    "duplicates drop date, force\n",
    "sort date\n",
    "\n",
    "* Save temporary GDP file\n",
    "save \"data/processed/temp_gdp.dta\", replace\n",
    "\n",
    "describe\n",
    "summarize gdp lngdp gdp_growth, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf93a75",
   "metadata": {},
   "source": [
    "### Processing Real GDP: Quarterly to Monthly Conversion\n",
    "\n",
    "**About the GDP Data (GDPC1):**\n",
    "- **Definition**: Real Gross Domestic Product\n",
    "- **Units**: Billions of Chained 2017 Dollars\n",
    "- **Frequency**: Quarterly (Q1, Q2, Q3, Q4)\n",
    "- **Seasonal adjustment**: Seasonally Adjusted Annual Rate (SAAR)\n",
    "- **Source**: U.S. Bureau of Economic Analysis via FRED\n",
    "\n",
    "**Why \"Real\" GDP?**\n",
    "- Adjusted for inflation (using 2017 as base year)\n",
    "- Measures actual output, not just price changes\n",
    "- Allows comparison across time periods\n",
    "- \"Chained\" means uses chain-weighted price index (more accurate than fixed weights)\n",
    "\n",
    "---\n",
    "\n",
    "**Quarterly Date Handling in Stata**\n",
    "\n",
    "**The `%tq` format:**\n",
    "- Quarterly dates stored as integers (quarters since 1960q1)\n",
    "- 1960q1 = 0, 1960q2 = 1, 1960q3 = 2, etc.\n",
    "- Display format: `%tq` shows as \"2020q1\", \"2020q2\", etc.\n",
    "\n",
    "**Date extraction functions:**\n",
    "```stata\n",
    "gen year = year(date_numeric)      // Extract year from daily date\n",
    "gen month = month(date_numeric)    // Extract month (1-12)\n",
    "gen quarter = ceil(month/3)        // Compute quarter (1-4)\n",
    "```\n",
    "\n",
    "**The `ceil()` function** (ceiling):\n",
    "- Rounds UP to nearest integer\n",
    "- January (month=1): ceil(1/3) = ceil(0.33) = 1 → Q1\n",
    "- February (month=2): ceil(2/3) = ceil(0.67) = 1 → Q1\n",
    "- March (month=3): ceil(3/3) = ceil(1.00) = 1 → Q1\n",
    "- April (month=4): ceil(4/3) = ceil(1.33) = 2 → Q2\n",
    "- etc.\n",
    "\n",
    "**Alternative:** `quarter()` function:\n",
    "```stata\n",
    "gen quarter = quarter(date_numeric)\n",
    "```\n",
    "This is simpler and equivalent!\n",
    "\n",
    "**The `yq()` function:**\n",
    "- Creates quarterly date from year and quarter\n",
    "- `yq(2020, 1)` creates 2020q1\n",
    "- Returns integer that Stata interprets as quarterly date when formatted with `%tq`\n",
    "\n",
    "---\n",
    "\n",
    "**GDP Growth Rate Calculation**\n",
    "\n",
    "**Theory: Why use growth rates?**\n",
    "\n",
    "**Level vs. Growth:**\n",
    "- **GDP level** ($Y_t$): Actual output (e.g., $23,000 billion)\n",
    "  - Non-stationary: has trend (economy grows over time)\n",
    "  - Scale-dependent: hard to compare across countries or time periods\n",
    "  \n",
    "- **GDP growth rate** ($g_t$): Percentage change\n",
    "  - Closer to stationary (though can have business cycle persistence)\n",
    "  - Scale-free: comparable across contexts\n",
    "  - What matters for economic analysis (expansions vs. recessions)\n",
    "\n",
    "**Computation:**\n",
    "\n",
    "**Discrete growth rate:**\n",
    "$$g_t = \\frac{Y_t - Y_{t-1}}{Y_{t-1}} = \\frac{Y_t}{Y_{t-1}} - 1$$\n",
    "\n",
    "**Log growth rate (our choice):**\n",
    "$$g_t = \\ln(Y_t) - \\ln(Y_{t-1}) = \\Delta \\ln(Y_t)$$\n",
    "\n",
    "**For small changes:** $\\Delta \\ln(Y_t) \\approx \\frac{Y_t - Y_{t-1}}{Y_{t-1}}$\n",
    "\n",
    "**Advantages of log growth:**\n",
    "- Time-additive: 4-quarter growth = sum of 4 quarterly growth rates\n",
    "- Symmetric (as discussed for returns)\n",
    "- More likely to be normally distributed\n",
    "\n",
    "**Annualized growth:**\n",
    "- Our calculation gives **quarterly growth** (e.g., 0.005 = 0.5% per quarter)\n",
    "- To annualize: multiply by 4 (e.g., 0.5% × 4 = 2% annual rate)\n",
    "- BEA reports \"annualized\" growth (compounded: $(1+g_q)^4 - 1$)\n",
    "- We keep quarterly for consistency with data frequency\n",
    "\n",
    "**Stata implementation:**\n",
    "```stata\n",
    "gen lngdp = ln(gdp)\n",
    "gen gdp_growth = D.lngdp\n",
    "```\n",
    "\n",
    "**Why `tsset` before creating growth:**\n",
    "- `D.` operator requires time series structure\n",
    "- Ensures correct time alignment (t vs. t-1)\n",
    "- Handles gaps properly (though quarterly GDP rarely has gaps)\n",
    "\n",
    "---\n",
    "\n",
    "**Temporary File Strategy**\n",
    "\n",
    "**Why save temporary files?**\n",
    "- Each series processed separately (clearer, easier to debug)\n",
    "- Then merged together at the end\n",
    "- Allows checking each series individually\n",
    "- Can rerun parts without reprocessing everything\n",
    "\n",
    "**The `save` command with temporary files:**\n",
    "```stata\n",
    "save \"data/processed/temp_gdp.dta\", replace\n",
    "```\n",
    "\n",
    "**File naming convention:**\n",
    "- `temp_`: Indicates file is intermediate, not final output\n",
    "- Will be deleted after merging\n",
    "- Clearly distinguishes from final processed data\n",
    "\n",
    "**Best practice:**\n",
    "- Process each series completely (import → clean → transform → save)\n",
    "- Verify each series individually\n",
    "- Merge all at the end\n",
    "- Clean up temporary files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ee542",
   "metadata": {},
   "source": [
    "### 5.2 CPI (Monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac4666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(encoding automatically selected: ISO-8859-1)\n",
      "(2 vars, 945 obs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time variable: date, 1947m1 to 2025m9\n",
      "        Delta: 1 month\n",
      "\n",
      "\n",
      "(1 missing value generated)\n",
      "\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "(0 observations are duplicates)\n",
      "\n",
      "\n",
      "(file data/processed/temp_cpi.dta not found)\n",
      "file data/processed/temp_cpi.dta saved\n",
      "\n",
      "\n",
      "Contains data from data/processed/temp_cpi.dta\n",
      " Observations:           945                  \n",
      "    Variables:             3                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "cpi             float   %9.0g                 CPIAUCSL\n",
      "date            float   %tm                   \n",
      "inflation       float   %9.0g                 \n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n",
      "\n",
      "\n",
      "                          CPIAUCSL\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%        22.91          21.48\n",
      " 5%        24.98          21.62\n",
      "10%        26.85          21.95       Obs                 945\n",
      "25%        32.75             22       Sum of wgt.         945\n",
      "\n",
      "50%        109.5                      Mean           123.4215\n",
      "                        Largest       Std. dev.      88.82391\n",
      "75%        199.3          321.5\n",
      "90%      247.284        322.132       Variance       7889.687\n",
      "95%      276.528        323.364       Skewness       .4530693\n",
      "99%      317.603        324.368       Kurtosis       1.927122\n",
      "\n",
      "                          inflation\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.5034924      -1.786423\n",
      " 5%    -.1638889      -.9239912\n",
      "10%    -.0447273      -.8635521       Obs                 944\n",
      "25%     .1005888      -.8267403       Sum of wgt.         944\n",
      "\n",
      "50%     .2473474                      Mean           .2875801\n",
      "                        Largest       Std. dev.      .3398177\n",
      "75%     .4373789       1.742363\n",
      "90%      .676918       1.757526       Variance       .1154761\n",
      "95%     .9370804       1.793766       Skewness       .5660575\n",
      "99%     1.382828       1.945233       Kurtosis       7.096109\n"
     ]
    }
   ],
   "source": [
    "* Load CPI data (monthly)\n",
    "import delimited \"data/raw/CPIAUCSL.csv\", clear\n",
    "\n",
    "* Parse dates from observation_date column\n",
    "gen date_numeric = date(observation_date, \"YMD\")\n",
    "drop observation_date\n",
    "\n",
    "* Convert to monthly format\n",
    "format date_numeric %td\n",
    "gen year = year(date_numeric)\n",
    "gen month = month(date_numeric)\n",
    "gen date_m = ym(year, month)\n",
    "format date_m %tm\n",
    "\n",
    "* Rename CPI variable (column name matches filename: CPIAUCSL)\n",
    "rename cpiaucsl cpi\n",
    "\n",
    "* Set time series (required before using D. operator)\n",
    "rename date_m date\n",
    "tsset date\n",
    "\n",
    "* Create inflation rate (monthly change, annualized percentage)\n",
    "* First create log of CPI, then take first difference\n",
    "gen ln_cpi = ln(cpi)\n",
    "gen inflation = 100 * D.ln_cpi\n",
    "\n",
    "* Keep only monthly date and variables\n",
    "keep date cpi inflation\n",
    "\n",
    "* Remove duplicates and sort\n",
    "duplicates drop date, force\n",
    "sort date\n",
    "\n",
    "* Save temporary CPI file\n",
    "save \"data/processed/temp_cpi.dta\", replace\n",
    "\n",
    "describe\n",
    "summarize cpi inflation, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4e688",
   "metadata": {},
   "source": [
    "### Processing CPI and Computing Inflation\n",
    "\n",
    "**About the CPI Data (CPIAUCSL):**\n",
    "- **Definition**: Consumer Price Index for All Urban Consumers: All Items\n",
    "- **Coverage**: U.S. City Average\n",
    "- **Base period**: Index where 1982-84 = 100\n",
    "- **Frequency**: Monthly\n",
    "- **Seasonal adjustment**: Seasonally Adjusted\n",
    "- **Source**: U.S. Bureau of Labor Statistics via FRED\n",
    "\n",
    "**What CPI measures:**\n",
    "- Average change in prices paid by urban consumers for a market basket of goods and services\n",
    "- Includes: food, clothing, shelter, fuel, transportation, medical care, etc.\n",
    "- Covers about 93% of U.S. population\n",
    "- Most widely used measure of inflation\n",
    "\n",
    "---\n",
    "\n",
    "**Monthly Date Format**\n",
    "\n",
    "**The `%tm` format:**\n",
    "- Monthly dates stored as integers (months since 1960m1)\n",
    "- 1960m1 = 0, 1960m2 = 1, etc.\n",
    "- Display format: `%tm` shows as \"2020m1\", \"2020m2\", etc.\n",
    "\n",
    "**The `ym()` function:**\n",
    "```stata\n",
    "gen date_m = ym(year, month)\n",
    "```\n",
    "- Combines year and month into Stata monthly date\n",
    "- `ym(2020, 3)` creates 2020m3 (March 2020)\n",
    "- Returns integer interpretable as monthly date with `%tm` format\n",
    "\n",
    "**Why extract year and month first:**\n",
    "- FRED gives daily date (even though CPI is monthly)\n",
    "- E.g., \"2020-03-01\" for March 2020 CPI\n",
    "- We extract components to create proper monthly date\n",
    "- Ensures all March values map to same 2020m3\n",
    "\n",
    "---\n",
    "\n",
    "**Inflation Calculation: Theory and Practice**\n",
    "\n",
    "**What is inflation?**\n",
    "\n",
    "Inflation ($\\pi_t$) is the rate of change in the price level.\n",
    "\n",
    "**Discrete inflation rate:**\n",
    "$$\\pi_t = \\frac{CPI_t - CPI_{t-1}}{CPI_{t-1}} \\times 100\\%$$\n",
    "\n",
    "**Log approximation (our method):**\n",
    "$$\\pi_t = 100 \\times (\\ln(CPI_t) - \\ln(CPI_{t-1})) = 100 \\times \\Delta \\ln(CPI_t)$$\n",
    "\n",
    "**Why these are approximately equal:**\n",
    "\n",
    "For small changes, using Taylor expansion:\n",
    "$$\\ln(1 + x) \\approx x \\text{ when } |x| \\text{ is small}$$\n",
    "\n",
    "Therefore:\n",
    "$$\\ln(CPI_t) - \\ln(CPI_{t-1}) = \\ln\\left(\\frac{CPI_t}{CPI_{t-1}}\\right) = \\ln\\left(1 + \\frac{CPI_t - CPI_{t-1}}{CPI_{t-1}}\\right) \\approx \\frac{CPI_t - CPI_{t-1}}{CPI_{t-1}}$$\n",
    "\n",
    "**Example:**\n",
    "- If CPI goes from 250 to 252.5 (1% increase):\n",
    "- Discrete: $(252.5 - 250)/250 = 0.01 = 1\\%$\n",
    "- Log: $\\ln(252.5) - \\ln(250) = 0.00995 \\approx 1\\%$\n",
    "\n",
    "**Why multiply by 100:**\n",
    "- Converts to percentage points\n",
    "- 0.01 × 100 = 1 (meaning 1%)\n",
    "- Standard presentation format\n",
    "- Easier interpretation\n",
    "\n",
    "**Monthly vs. Annualized inflation:**\n",
    "- Our calculation: **monthly inflation rate** (percentage change from previous month)\n",
    "- To annualize (roughly): multiply by 12\n",
    "- More precisely: $(1 + \\pi_{monthly})^{12} - 1$\n",
    "- Federal Reserve targets 2% **annual** inflation (≈ 0.167% monthly)\n",
    "\n",
    "**Stata implementation:**\n",
    "```stata\n",
    "gen ln_cpi = ln(cpi)\n",
    "gen inflation = 100 * D.ln_cpi\n",
    "```\n",
    "\n",
    "**Step by step:**\n",
    "1. `ln(cpi)`: Take natural log of CPI index\n",
    "2. `D.ln_cpi`: Compute first difference (requires `tsset` first)\n",
    "3. `100 *`: Convert to percentage points\n",
    "\n",
    "**Why take log first, then difference:**\n",
    "- `D.ln(cpi)` = $\\ln(CPI_t) - \\ln(CPI_{t-1})$ = log change\n",
    "- This is NOT the same as `ln(D.cpi)` = $\\ln(CPI_t - CPI_{t-1})$ (not useful!)\n",
    "- Order matters: log-difference ≠ difference-log\n",
    "\n",
    "---\n",
    "\n",
    "**Interpreting CPI and Inflation**\n",
    "\n",
    "**Expected patterns:**\n",
    "\n",
    "**For CPI (level):**\n",
    "- **Always increasing** (except rare deflation periods)\n",
    "- Value around 250-300 in recent years (base 1982-84 = 100)\n",
    "- Non-stationary: has a trend\n",
    "\n",
    "**For inflation (first difference):**\n",
    "- **Mean**: Around 0.15-0.25% monthly (2-3% annualized)\n",
    "- **Range**: Typically -0.5% to +1% monthly\n",
    "  - Negative: deflation (rare in U.S. since 1950s)\n",
    "  - > 1% monthly: high inflation period (1970s, early 1980s, 2021-2022)\n",
    "- **Volatility**: Varies over time\n",
    "  - \"Great Moderation\" (1990s-2007): low, stable inflation\n",
    "  - Recent years: more volatile\n",
    "- **Persistence**: Inflation is autocorrelated (past inflation predicts future)\n",
    "\n",
    "**Uses in macroeconomic analysis:**\n",
    "- **Monetary policy**: Fed targets inflation around 2% annual\n",
    "- **Real vs. nominal**: Deflate nominal variables using CPI\n",
    "- **VAR analysis**: Inflation as endogenous variable with interest rates, GDP\n",
    "- **Phillips curve**: Relationship between inflation and unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb2613",
   "metadata": {},
   "source": [
    "### 5.3 Interest Rate (Monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746c35b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(encoding automatically selected: ISO-8859-1)\n",
      "(2 vars, 856 obs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "(0 observations are duplicates)\n",
      "\n",
      "\n",
      "(file data/processed/temp_interest.dta not found)\n",
      "file data/processed/temp_interest.dta saved\n",
      "\n",
      "\n",
      "Contains data from data/processed/temp_interest.dta\n",
      " Observations:           856                  \n",
      "    Variables:             2                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "interest_rate   float   %9.0g                 FEDFUNDS\n",
      "date            float   %tm                   \n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n",
      "\n",
      "\n",
      "                          FEDFUNDS\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%          .07            .05\n",
      " 5%          .09            .05\n",
      "10%          .16            .06       Obs                 856\n",
      "25%        1.895            .07       Sum of wgt.         856\n",
      "\n",
      "50%         4.33                      Mean           4.605678\n",
      "                        Largest       Std. dev.      3.546838\n",
      "75%        6.135           18.9\n",
      "90%         9.24          19.04       Variance       12.58006\n",
      "95%        10.78          19.08       Skewness       1.073105\n",
      "99%        15.93           19.1       Kurtosis       4.691378\n"
     ]
    }
   ],
   "source": [
    "* Load interest rate data (Fed Funds Rate - monthly)\n",
    "import delimited \"data/raw/FEDFUNDS.csv\", clear\n",
    "\n",
    "* Parse dates from observation_date column\n",
    "gen date_numeric = date(observation_date, \"YMD\")\n",
    "drop observation_date\n",
    "\n",
    "* Convert to monthly format\n",
    "format date_numeric %td\n",
    "gen year = year(date_numeric)\n",
    "gen month = month(date_numeric)\n",
    "gen date_m = ym(year, month)\n",
    "format date_m %tm\n",
    "\n",
    "* Rename interest rate variable (column name matches filename: FEDFUNDS)\n",
    "rename fedfunds interest_rate\n",
    "\n",
    "* Keep only monthly date and variables\n",
    "keep date_m interest_rate\n",
    "\n",
    "* Rename date variable\n",
    "rename date_m date\n",
    "\n",
    "* Remove duplicates and sort\n",
    "duplicates drop date, force\n",
    "sort date\n",
    "\n",
    "* Save temporary interest rate file\n",
    "save \"data/processed/temp_interest.dta\", replace\n",
    "\n",
    "describe\n",
    "summarize interest_rate, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aedab7",
   "metadata": {},
   "source": [
    "### Processing Interest Rates\n",
    "\n",
    "**About FEDFUNDS:**\n",
    "- **Definition**: Effective Federal Funds Rate\n",
    "- **Units**: Percent per annum\n",
    "- **Frequency**: Monthly (average of daily values)\n",
    "- **Source**: Federal Reserve Board via FRED\n",
    "\n",
    "**What is the Federal Funds Rate?**\n",
    "\n",
    "The federal funds rate is the **interest rate** at which depository institutions (banks and credit unions) lend reserve balances to other depository institutions overnight.\n",
    "\n",
    "**Key characteristics:**\n",
    "- **Overnight rate**: Very short-term (1-day) lending\n",
    "- **Interbank market**: Banks lending to each other\n",
    "- **Unsecured**: No collateral required\n",
    "- **Market-determined**: Result of supply and demand (but heavily influenced by Fed)\n",
    "\n",
    "**Why it matters:**\n",
    "- **Monetary policy instrument**: Fed's primary tool for controlling monetary conditions\n",
    "- **Target rate**: FOMC sets a target range, market rate gravitates toward it\n",
    "- **Benchmark**: Influences all other interest rates in the economy\n",
    "  - Prime rate = Fed Funds + ~3%\n",
    "  - Mortgage rates, corporate bonds, etc. move with Fed Funds\n",
    "- **Economic indicator**: Low rates → stimulus, High rates → restraint\n",
    "\n",
    "**Historical context:**\n",
    "- Near 20% in early 1980s (Volcker fighting inflation)\n",
    "- Near 0% in 2008-2015 (Great Recession response)\n",
    "- Near 0% in 2020-2021 (COVID-19 response)\n",
    "- Rapid increases in 2022-2023 (fighting inflation)\n",
    "\n",
    "**No transformation needed:**\n",
    "- Already in interpretable units (%)\n",
    "- Already stationary (tends to mean-revert, though slowly)\n",
    "- Use in levels for VAR analysis\n",
    "\n",
    "**For time series analysis:**\n",
    "- Interest rates typically $I(1)$ or near unit root (high persistence)\n",
    "- First difference would be change in interest rate (useful for some models)\n",
    "- We keep the level for standard macro VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39226571",
   "metadata": {},
   "source": [
    "### 5.4 Unemployment Rate (Monthly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(encoding automatically selected: ISO-8859-1)\n",
      "(2 vars, 932 obs)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Duplicates in terms of date\n",
      "\n",
      "(0 observations are duplicates)\n",
      "\n",
      "\n",
      "(file data/processed/temp_unemployment.dta not found)\n",
      "file data/processed/temp_unemployment.dta saved\n",
      "\n",
      "\n",
      "Contains data from data/processed/temp_unemployment.dta\n",
      " Observations:           932                  \n",
      "    Variables:             2                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "unemployment    float   %9.0g                 UNRATE\n",
      "date            float   %tm                   \n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n",
      "\n",
      "\n",
      "                           UNRATE\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%          2.9            2.5\n",
      " 5%          3.5            2.5\n",
      "10%          3.7            2.6       Obs                 932\n",
      "25%          4.3            2.6       Sum of wgt.         932\n",
      "\n",
      "50%          5.5                      Mean           5.670386\n",
      "                        Largest       Std. dev.      1.707418\n",
      "75%          6.7           10.8\n",
      "90%          7.8             11       Variance       2.915277\n",
      "95%            9           13.2       Skewness       .8738871\n",
      "99%         10.2           14.8       Kurtosis       4.073464\n"
     ]
    }
   ],
   "source": [
    "* Load unemployment rate data (monthly)\n",
    "import delimited \"data/raw/UNRATE.csv\", clear\n",
    "\n",
    "* Parse dates from observation_date column\n",
    "gen date_numeric = date(observation_date, \"YMD\")\n",
    "drop observation_date\n",
    "\n",
    "* Convert to monthly format\n",
    "format date_numeric %td\n",
    "gen year = year(date_numeric)\n",
    "gen month = month(date_numeric)\n",
    "gen date_m = ym(year, month)\n",
    "format date_m %tm\n",
    "\n",
    "* Rename unemployment variable (column name matches filename: UNRATE)\n",
    "rename unrate unemployment\n",
    "\n",
    "* Keep only monthly date and variables\n",
    "keep date_m unemployment\n",
    "\n",
    "* Rename date variable\n",
    "rename date_m date\n",
    "\n",
    "* Remove duplicates and sort\n",
    "duplicates drop date, force\n",
    "sort date\n",
    "\n",
    "* Save temporary unemployment file\n",
    "save \"data/processed/temp_unemployment.dta\", replace\n",
    "\n",
    "describe\n",
    "summarize unemployment, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de812bbb",
   "metadata": {},
   "source": [
    "### Processing Unemployment Rate\n",
    "\n",
    "**About UNRATE:**\n",
    "- **Definition**: Civilian Unemployment Rate\n",
    "- **Units**: Percent of labor force\n",
    "- **Frequency**: Monthly\n",
    "- **Seasonal adjustment**: Seasonally Adjusted\n",
    "- **Source**: U.S. Bureau of Labor Statistics via FRED\n",
    "\n",
    "**What is the unemployment rate?**\n",
    "\n",
    "$$\\text{Unemployment Rate} = \\frac{\\text{Number of Unemployed}}{\\text{Labor Force}} \\times 100\\%$$\n",
    "\n",
    "Where:\n",
    "- **Unemployed**: People without a job, actively looking for work, and available to work\n",
    "- **Labor Force**: Employed + Unemployed (excludes those not looking for work)\n",
    "\n",
    "**Key characteristics:**\n",
    "- **Countercyclical**: Rises in recessions, falls in expansions\n",
    "- **Lagging indicator**: Often continues rising after recession ends\n",
    "- **Natural rate**: Long-run average around 4-5% in U.S. (varies by period)\n",
    "- **Structural component**: Some unemployment always exists (job search, mismatch)\n",
    "\n",
    "**Historical patterns:**\n",
    "- Low: ~3.5% (late 1960s, 2019)\n",
    "- High: ~10% (Great Recession 2009, early 1980s)\n",
    "- COVID shock: Spiked to ~14.7% in April 2020\n",
    "\n",
    "**Use in macroeconomic analysis:**\n",
    "\n",
    "1. **Phillips Curve**: Relationship between unemployment and inflation\n",
    "   - Original: $\\pi_t = f(u_t)$ - inverse relationship\n",
    "   - Expectations-augmented: $\\pi_t = \\pi_t^e - \\alpha(u_t - u_t^*)$\n",
    "   - Where $u_t^*$ is natural rate (NAIRU)\n",
    "\n",
    "2. **Okun's Law**: Relationship between GDP growth and unemployment changes\n",
    "   - Approximately: $\\Delta u_t \\approx -0.5 \\times (g_t - 3\\%)$\n",
    "   - GDP growth above trend → unemployment falls\n",
    "\n",
    "3. **Monetary policy**: Fed's dual mandate (price stability + maximum employment)\n",
    "   - Fed responds to unemployment in setting interest rates\n",
    "   - VAR with unemployment, inflation, interest rates captures policy response\n",
    "\n",
    "**No transformation needed:**\n",
    "- Already in percentage points (interpretable units)\n",
    "- Relatively stationary around time-varying natural rate\n",
    "- Use in levels for VAR (though some researchers difference it)\n",
    "\n",
    "**Expected statistics:**\n",
    "- **Mean**: Depends on sample period (4-6% typical)\n",
    "- **Range**: 3% to 15% (covering normal times and severe recessions)\n",
    "- **Persistence**: Highly autocorrelated (changes slowly)\n",
    "- **Seasonal adjustment**: Important (unemployment varies by season)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390d92c",
   "metadata": {},
   "source": [
    "### 5.5 Stock Index Returns (Monthly from S&P 500)\n",
    "\n",
    "Aggregate daily S&P 500 data to monthly returns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e5bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(2,488 observations deleted)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(file data/processed/temp_stock.dta not found)\n",
      "file data/processed/temp_stock.dta saved\n",
      "\n",
      "\n",
      "Contains data from data/processed/temp_stock.dta\n",
      " Observations:           121                  \n",
      "    Variables:             2                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "date            float   %tm                   \n",
      "stock_return    float   %9.0g                 \n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n",
      "\n",
      "\n",
      "                        stock_return\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     -.100915      -.1157656\n",
      " 5%    -.0583563       -.100915\n",
      "10%     -.043582       -.088582       Obs                 121\n",
      "25%    -.0116663      -.0724497       Sum of wgt.         121\n",
      "\n",
      "50%     .0108213                      Mean           .0084696\n",
      "                        Largest       Std. dev.      .0380827\n",
      "75%     .0283365       .0785065\n",
      "90%     .0627522       .0787125       Variance       .0014503\n",
      "95%     .0658751       .0948958       Skewness      -.4427581\n",
      "99%     .0948958       .1042986       Kurtosis       3.897131\n"
     ]
    }
   ],
   "source": [
    "* Load processed S&P 500 data\n",
    "use \"data/processed/sp500_data.dta\", clear\n",
    "\n",
    "* Convert daily date to monthly\n",
    "gen year = year(date)\n",
    "gen month = month(date)\n",
    "gen date_m = ym(year, month)\n",
    "format date_m %tm\n",
    "\n",
    "* Aggregate to monthly: take last observation of each month\n",
    "* For returns, we can sum daily returns to get monthly returns\n",
    "bysort date_m: egen stock_return = total(sp500_return)\n",
    "\n",
    "* Keep only one observation per month (last day of month)\n",
    "bysort date_m: keep if _n == _N\n",
    "\n",
    "* Keep only monthly date and return\n",
    "keep date_m stock_return\n",
    "\n",
    "* Rename date variable\n",
    "rename date_m date\n",
    "\n",
    "* Sort\n",
    "sort date\n",
    "\n",
    "* Save temporary stock return file\n",
    "save \"data/processed/temp_stock.dta\", replace\n",
    "\n",
    "describe\n",
    "summarize stock_return, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49d444",
   "metadata": {},
   "source": [
    "### Aggregating Daily Returns to Monthly Frequency\n",
    "\n",
    "Now we need to convert the S&P 500 daily data to monthly frequency to match our macroeconomic variables.\n",
    "\n",
    "**The challenge:**\n",
    "- We have daily stock returns (about 252 trading days/year)\n",
    "- We need monthly returns (12 observations/year)\n",
    "- How do we aggregate?\n",
    "\n",
    "---\n",
    "\n",
    "**Aggregation Methods for Returns**\n",
    "\n",
    "**Option 1: End-of-month prices (Discrete return)**\n",
    "- Take last price of month $t$ and last price of month $t-1$\n",
    "- Compute: $R_{monthly} = \\frac{P_{end,t} - P_{end,t-1}}{P_{end,t-1}}$\n",
    "- **Pros**: Simple, uses actual observed prices\n",
    "- **Cons**: Ignores intra-month information\n",
    "\n",
    "**Option 2: Compound daily returns (Discrete)**\n",
    "- Product of daily gross returns: $(1+r_1)(1+r_2)...(1+r_n) - 1$\n",
    "- **Pros**: Accurate if returns are simple returns\n",
    "- **Cons**: Complicated, compounding errors\n",
    "\n",
    "**Option 3: Sum log returns (Our method)**\n",
    "- Sum daily log returns: $r_{monthly} = \\sum_{i=1}^{n} r_{daily,i}$\n",
    "- **Pros**: \n",
    "  - Mathematically correct for log returns (time-additive property!)\n",
    "  - Uses all available information\n",
    "  - Simple implementation\n",
    "- **Cons**: None for log returns\n",
    "\n",
    "---\n",
    "\n",
    "**Why Summing Log Returns Works**\n",
    "\n",
    "**Mathematical property of log returns:**\n",
    "\n",
    "$$r_t = \\ln(P_t) - \\ln(P_{t-1})$$\n",
    "\n",
    "For multiple periods:\n",
    "$$r_{1,n} = \\ln(P_n) - \\ln(P_1) = \\sum_{i=1}^{n} (\\ln(P_i) - \\ln(P_{i-1})) = \\sum_{i=1}^{n} r_i$$\n",
    "\n",
    "**Example:** March 2020 with 21 trading days\n",
    "- Day 1: $r_1 = 0.02$ (2% gain)\n",
    "- Day 2: $r_2 = -0.03$ (-3% loss)\n",
    "- ...\n",
    "- Day 21: $r_{21} = 0.01$ (1% gain)\n",
    "\n",
    "**Monthly return:**\n",
    "$$r_{March} = r_1 + r_2 + ... + r_{21}$$\n",
    "\n",
    "This equals $\\ln(P_{March31}) - \\ln(P_{Feb28})$, exactly what we want!\n",
    "\n",
    "---\n",
    "\n",
    "**Stata Implementation**\n",
    "\n",
    "**Step 1: Create monthly date from daily date**\n",
    "```stata\n",
    "gen year = year(date)\n",
    "gen month = month(date)\n",
    "gen date_m = ym(year, month)\n",
    "format date_m %tm\n",
    "```\n",
    "\n",
    "**Step 2: Aggregate returns within each month**\n",
    "```stata\n",
    "bysort date_m: egen stock_return = total(sp500_return)\n",
    "```\n",
    "\n",
    "**The `bysort` prefix:**\n",
    "- `bysort varlist:` performs command separately for each value of varlist\n",
    "- Here: operates on each unique value of `date_m` (each month)\n",
    "- Equivalent to looping over months, but more efficient\n",
    "\n",
    "**The `egen` command with `total()` function:**\n",
    "- `egen`: Extended generate - creates variables using functions\n",
    "- `total()`: Sums values within each group\n",
    "- Different from `sum()` (running sum)\n",
    "\n",
    "**What happens:**\n",
    "- For each month (e.g., 2020m3):\n",
    "  - Finds all daily observations in that month (21 trading days)\n",
    "  - Sums their returns: $r_1 + r_2 + ... + r_{21}$\n",
    "  - Assigns this sum to all observations in that month\n",
    "  - Result: all 21 days have same `stock_return` value\n",
    "\n",
    "**Step 3: Keep one observation per month**\n",
    "```stata\n",
    "bysort date_m: keep if _n == _N\n",
    "```\n",
    "\n",
    "**Understanding `_n` and `_N`:**\n",
    "- `_n`: Current observation number within group (1, 2, 3, ...)\n",
    "- `_N`: Total number of observations in group\n",
    "\n",
    "**What `_n == _N` means:**\n",
    "- Keep only if current obs number equals total obs in group\n",
    "- I.e., keep the **last observation** in each month\n",
    "- Result: One observation per month (the last trading day)\n",
    "\n",
    "**Alternative (equivalent):**\n",
    "```stata\n",
    "bysort date_m: keep if _n == 1  // Keep first day of month\n",
    "```\n",
    "Doesn't matter which day we keep since they all have the same aggregated return value!\n",
    "\n",
    "---\n",
    "\n",
    "**Expected Results**\n",
    "\n",
    "After aggregation:\n",
    "- ~12 observations per year instead of ~252\n",
    "- Monthly returns are larger magnitude than daily (more time to accumulate)\n",
    "- Typical monthly return: -5% to +10% (much wider range than daily)\n",
    "- Monthly volatility: ~3-5% standard deviation\n",
    "- Still approximately normally distributed (but with fat tails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed8134",
   "metadata": {},
   "source": [
    "### 5.6 Merge All Macro-Finance Series\n",
    "\n",
    "Merge all monthly series together. Note: GDP is quarterly, so we'll merge it separately or convert to monthly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131725d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "(628 observations created)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(930 observations deleted)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(file data/processed/temp_gdp_monthly.dta not found)\n",
      "file data/processed/temp_gdp_monthly.dta saved\n",
      "\n",
      "\n",
      "\n",
      "    Result                      Number of obs\n",
      "    -----------------------------------------\n",
      "    Not matched                            91\n",
      "        from master                        90  \n",
      "        from using                          1  \n",
      "\n",
      "    Matched                               855  \n",
      "    -----------------------------------------\n",
      "\n",
      "\n",
      "    Result                      Number of obs\n",
      "    -----------------------------------------\n",
      "    Not matched                            14\n",
      "        from master                        14  \n",
      "        from using                          0  \n",
      "\n",
      "    Matched                               932  \n",
      "    -----------------------------------------\n",
      "\n",
      "\n",
      "    Result                      Number of obs\n",
      "    -----------------------------------------\n",
      "    Not matched                           827\n",
      "        from master                       826  \n",
      "        from using                          1  \n",
      "\n",
      "    Matched                               120  \n",
      "    -----------------------------------------\n",
      "\n",
      "\n",
      "    Result                      Number of obs\n",
      "    -----------------------------------------\n",
      "    Not matched                           935\n",
      "        from master                       935  \n",
      "        from using                          0  \n",
      "\n",
      "    Matched                                12  \n",
      "    -----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "* First, process GDP quarterly to monthly conversion\n",
    "* Load GDP quarterly data\n",
    "use \"data/processed/temp_gdp.dta\", clear\n",
    "\n",
    "* Extract year and quarter from quarterly date\n",
    "gen year = year(date)\n",
    "gen quarter = quarter(date)\n",
    "\n",
    "* Create monthly dates for all three months in each quarter\n",
    "expand 3\n",
    "bysort date: gen month_in_quarter = _n  // 1, 2, or 3\n",
    "gen month = (quarter - 1) * 3 + month_in_quarter\n",
    "gen date_m = ym(year, month)\n",
    "format date_m %tm\n",
    "\n",
    "* Keep only one observation per month (carry forward quarterly values)\n",
    "bysort date_m: keep if _n == 1\n",
    "\n",
    "* Drop old quarterly date variable before renaming\n",
    "drop date\n",
    "\n",
    "* Rename and keep variables\n",
    "rename date_m date\n",
    "keep date gdp lngdp gdp_growth\n",
    "\n",
    "* Sort by date\n",
    "sort date\n",
    "\n",
    "* Save temporary expanded GDP\n",
    "save \"data/processed/temp_gdp_monthly.dta\", replace\n",
    "\n",
    "* Now start with CPI as base (monthly) and merge all series\n",
    "use \"data/processed/temp_cpi.dta\", clear\n",
    "\n",
    "* Merge with interest rate\n",
    "merge 1:1 date using \"data/processed/temp_interest.dta\", nogen\n",
    "\n",
    "* Merge with unemployment\n",
    "merge 1:1 date using \"data/processed/temp_unemployment.dta\", nogen\n",
    "\n",
    "* Merge with stock returns\n",
    "merge 1:1 date using \"data/processed/temp_stock.dta\", nogen\n",
    "\n",
    "* Merge with GDP (monthly version)\n",
    "merge 1:1 date using \"data/processed/temp_gdp_monthly.dta\", nogen\n",
    "\n",
    "* Sort by date\n",
    "sort date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7cb6da",
   "metadata": {},
   "source": [
    "### Merging Multiple Time Series: A Two-Step Process\n",
    "\n",
    "We now have five temporary datasets:\n",
    "1. `temp_gdp.dta` (quarterly)\n",
    "2. `temp_cpi.dta` (monthly)\n",
    "3. `temp_interest.dta` (monthly)\n",
    "4. `temp_unemployment.dta` (monthly)\n",
    "5. `temp_stock.dta` (monthly)\n",
    "\n",
    "Our goal: Create **one monthly dataset** with all variables aligned by date.\n",
    "\n",
    "**The problem:** GDP is quarterly, others are monthly. We need to handle this carefully.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1: Convert Quarterly GDP to Monthly**\n",
    "\n",
    "**The challenge:**\n",
    "- GDP: 4 observations per year (2020q1, 2020q2, 2020q3, 2020q4)\n",
    "- Others: 12 observations per year (2020m1, 2020m2, ..., 2020m12)\n",
    "\n",
    "**Options for quarterly → monthly conversion:**\n",
    "\n",
    "1. **Interpolation**: Create intermediate values between quarters\n",
    "   - Linear: Draw straight line between quarters\n",
    "   - Cubic spline: Smooth curve through quarters\n",
    "   - **Pros**: Creates realistic monthly values\n",
    "   - **Cons**: Introduces artificial variation not in original data\n",
    "\n",
    "2. **Aggregation markers**: Missing for 2 of 3 months, value in 3rd\n",
    "   - E.g., Q1 GDP only in March (Jan, Feb missing)\n",
    "   - **Pros**: Doesn't create fake data\n",
    "   - **Cons**: Many missing values, VAR can't handle\n",
    "\n",
    "3. **Carry forward (our choice)**: Repeat quarterly value for all 3 months\n",
    "   - E.g., 2020q1 GDP → same value for Jan, Feb, March\n",
    "   - **Pros**: No artificial variation, all months have data\n",
    "   - **Cons**: Ignores within-quarter dynamics, overstates persistence\n",
    "\n",
    "**Our implementation:**\n",
    "\n",
    "```stata\n",
    "expand 3\n",
    "```\n",
    "\n",
    "**The `expand` command:**\n",
    "- **Syntax**: `expand exp` or `expand varname`\n",
    "- **What it does**: Duplicates each observation `exp` times\n",
    "- **Our use**: Each quarterly observation becomes 3 monthly observations\n",
    "- **Example**:\n",
    "  - Before: 1 obs for 2020q1 (GDP = 19,000)\n",
    "  - After: 3 obs for 2020q1 (all with GDP = 19,000)\n",
    "\n",
    "**Creating the month identifier:**\n",
    "```stata\n",
    "bysort date: gen month_in_quarter = _n\n",
    "```\n",
    "- `_n` within each quarter: 1, 2, 3\n",
    "- Identifies which month within the quarter\n",
    "\n",
    "**Computing the actual month:**\n",
    "```stata\n",
    "gen month = (quarter - 1) * 3 + month_in_quarter\n",
    "```\n",
    "\n",
    "**Logic:**\n",
    "- Q1 (quarter=1): months 1, 2, 3 (Jan, Feb, Mar)\n",
    "  - month = (1-1)*3 + 1 = 1 (Jan)\n",
    "  - month = (1-1)*3 + 2 = 2 (Feb)\n",
    "  - month = (1-1)*3 + 3 = 3 (Mar)\n",
    "- Q2 (quarter=2): months 4, 5, 6 (Apr, May, Jun)\n",
    "  - month = (2-1)*3 + 1 = 4 (Apr)\n",
    "  - month = (2-1)*3 + 2 = 5 (May)\n",
    "  - month = (2-1)*3 + 3 = 6 (Jun)\n",
    "- Etc.\n",
    "\n",
    "**Create monthly date:**\n",
    "```stata\n",
    "gen date_m = ym(year, month)\n",
    "```\n",
    "\n",
    "**Keep one obs per month:**\n",
    "```stata\n",
    "bysort date_m: keep if _n == 1\n",
    "```\n",
    "- After `expand`, we might have duplicates within a month\n",
    "- This ensures exactly one observation per month\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Merge All Monthly Datasets**\n",
    "\n",
    "**The `merge` Command**\n",
    "\n",
    "**Syntax:**\n",
    "```stata\n",
    "merge join_type varlist using filename [, options]\n",
    "```\n",
    "\n",
    "**Join types:**\n",
    "- **`1:1`**: One-to-one (our case)\n",
    "  - Each value in master matches at most one in using\n",
    "  - Each value in using matches at most one in master\n",
    "  - Perfect for time series: one observation per date\n",
    "  \n",
    "- **`1:m`**: One-to-many\n",
    "  - One master observation matches multiple using observations\n",
    "  - Example: One country, many years\n",
    "  \n",
    "- **`m:1`**: Many-to-one\n",
    "  - Multiple master observations match one using observation\n",
    "  - Example: Many individuals in one region\n",
    "  \n",
    "- **`m:m`**: Many-to-many (rarely used, discouraged)\n",
    "\n",
    "**Our merges:**\n",
    "```stata\n",
    "merge 1:1 date using \"data/processed/temp_interest.dta\", nogen\n",
    "```\n",
    "\n",
    "**The `nogen` option:**\n",
    "- By default, `merge` creates `_merge` variable showing merge results:\n",
    "  - `_merge == 1`: obs in master only (not in using)\n",
    "  - `_merge == 2`: obs in using only (not in master)\n",
    "  - `_merge == 3`: obs in both (matched)\n",
    "- `nogen`: Don't create `_merge` (we're confident about our merges)\n",
    "\n",
    "**Alternative (checking merge):**\n",
    "```stata\n",
    "merge 1:1 date using \"temp_interest.dta\"\n",
    "tab _merge  // Check merge results\n",
    "drop _merge\n",
    "```\n",
    "\n",
    "**Sequential merging strategy:**\n",
    "1. Start with CPI (complete monthly coverage)\n",
    "2. Merge interest rates (adds `interest_rate` variable)\n",
    "3. Merge unemployment (adds `unemployment` variable)\n",
    "4. Merge stock returns (adds `stock_return` variable)\n",
    "5. Merge GDP (adds `gdp`, `lngdp`, `gdp_growth` variables)\n",
    "\n",
    "**Why this order?**\n",
    "- Start with most complete series (CPI)\n",
    "- Each merge adds variables (columns), not observations (rows)\n",
    "- Final dataset has dates where ALL series available\n",
    "\n",
    "**Handling mismatched dates:**\n",
    "- CPI: 1947-present (most complete)\n",
    "- Interest rates: 1954-present\n",
    "- Stock: 1950-present\n",
    "- GDP: 1947-present\n",
    "- Unemployment: 1948-present\n",
    "\n",
    "**Result:** \n",
    "- Merged dataset starts when ALL series available (1954-ish)\n",
    "- Earlier dates (only in CPI) will have missing values for other vars\n",
    "- Later dates: all variables present\n",
    "\n",
    "---\n",
    "\n",
    "**Understanding the Final Dataset**\n",
    "\n",
    "After merging:\n",
    "- **Time dimension**: Monthly observations from earliest common date to latest\n",
    "- **Cross-section**: 7 variables (date, cpi, inflation, interest_rate, unemployment, stock_return, gdp variables)\n",
    "- **Balanced panel**: All variables observed for all dates (or clearly missing)\n",
    "- **Ready for VAR**: Same frequency, aligned dates, complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time variable: date, 1947m1 to 2025m11\n",
      "        Delta: 1 month\n",
      "\n",
      "\n",
      "Time variable: date, 1947m1 to 2025m11\n",
      "        Delta: 1 month\n",
      "\n",
      "\n",
      "Contains data from data/processed/temp_cpi.dta\n",
      " Observations:           947                  \n",
      "    Variables:             9                  5 Nov 2025 01:24\n",
      "--------------------------------------------------------------------------------\n",
      "Variable      Storage   Display    Value\n",
      "    name         type    format    label      Variable label\n",
      "--------------------------------------------------------------------------------\n",
      "cpi             float   %9.0g                 Consumer Price Index\n",
      "date            float   %tm                   Date (Monthly)\n",
      "inflation       float   %9.0g                 Inflation Rate (Monthly % Change)\n",
      "interest_rate   float   %9.0g                 Interest Rate (%)\n",
      "unemployment    float   %9.0g                 Unemployment Rate (%)\n",
      "stock_return    float   %9.0g                 Stock Market Return (Monthly)\n",
      "gdp             float   %9.0g                 Real GDP (Quarterly, interpolated)\n",
      "lngdp           float   %9.0g                 Log Real GDP\n",
      "gdp_growth      float   %9.0g                 GDP Growth Rate (Quarterly)\n",
      "--------------------------------------------------------------------------------\n",
      "Sorted by: date\n",
      "     Note: Dataset has changed since last saved.\n",
      "\n",
      "\n",
      "                    Consumer Price Index\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%        22.91          21.48\n",
      " 5%        24.98          21.62\n",
      "10%        26.85          21.95       Obs                 945\n",
      "25%        32.75             22       Sum of wgt.         945\n",
      "\n",
      "50%        109.5                      Mean           123.4215\n",
      "                        Largest       Std. dev.      88.82391\n",
      "75%        199.3          321.5\n",
      "90%      247.284        322.132       Variance       7889.687\n",
      "95%      276.528        323.364       Skewness       .4530693\n",
      "99%      317.603        324.368       Kurtosis       1.927122\n",
      "\n",
      "                       Date (Monthly)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%         -147           -156\n",
      " 5%         -109           -155\n",
      "10%          -62           -154       Obs                 947\n",
      "25%           80           -153       Sum of wgt.         947\n",
      "\n",
      "50%          317                      Mean                317\n",
      "                        Largest       Std. dev.      273.5197\n",
      "75%          554            787\n",
      "90%          696            788       Variance          74813\n",
      "95%          743            789       Skewness              0\n",
      "99%          781            790       Kurtosis       1.799997\n",
      "\n",
      "              Inflation Rate (Monthly % Change)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.5034924      -1.786423\n",
      " 5%    -.1638889      -.9239912\n",
      "10%    -.0447273      -.8635521       Obs                 944\n",
      "25%     .1005888      -.8267403       Sum of wgt.         944\n",
      "\n",
      "50%     .2473474                      Mean           .2875801\n",
      "                        Largest       Std. dev.      .3398177\n",
      "75%     .4373789       1.742363\n",
      "90%      .676918       1.757526       Variance       .1154761\n",
      "95%     .9370804       1.793766       Skewness       .5660575\n",
      "99%     1.382828       1.945233       Kurtosis       7.096109\n",
      "\n",
      "                      Interest Rate (%)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%          .07            .05\n",
      " 5%          .09            .05\n",
      "10%          .16            .06       Obs                 856\n",
      "25%        1.895            .07       Sum of wgt.         856\n",
      "\n",
      "50%         4.33                      Mean           4.605678\n",
      "                        Largest       Std. dev.      3.546838\n",
      "75%        6.135           18.9\n",
      "90%         9.24          19.04       Variance       12.58006\n",
      "95%        10.78          19.08       Skewness       1.073105\n",
      "99%        15.93           19.1       Kurtosis       4.691378\n",
      "\n",
      "                    Unemployment Rate (%)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%          2.9            2.5\n",
      " 5%          3.5            2.5\n",
      "10%          3.7            2.6       Obs                 932\n",
      "25%          4.3            2.6       Sum of wgt.         932\n",
      "\n",
      "50%          5.5                      Mean           5.670386\n",
      "                        Largest       Std. dev.      1.707418\n",
      "75%          6.7           10.8\n",
      "90%          7.8             11       Variance       2.915277\n",
      "95%            9           13.2       Skewness       .8738871\n",
      "99%         10.2           14.8       Kurtosis       4.073464\n",
      "\n",
      "                Stock Market Return (Monthly)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     -.100915      -.1157656\n",
      " 5%    -.0583563       -.100915\n",
      "10%     -.043582       -.088582       Obs                 121\n",
      "25%    -.0116663      -.0724497       Sum of wgt.         121\n",
      "\n",
      "50%     .0108213                      Mean           .0084696\n",
      "                        Largest       Std. dev.      .0380827\n",
      "75%     .0283365       .0785065\n",
      "90%     .0627522       .0787125       Variance       .0014503\n",
      "95%     .0658751       .0948958       Skewness      -.4427581\n",
      "99%     .0948958       .1042986       Kurtosis       3.897131\n",
      "\n",
      "             Real GDP (Quarterly, interpolated)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     2239.682       2239.682\n",
      " 5%     2239.682       2239.682\n",
      "10%     2239.682       3141.224       Obs                  12\n",
      "25%     3381.238       3621.252       Sum of wgt.          12\n",
      "\n",
      "50%     7688.202                      Mean            10021.6\n",
      "                        Largest       Std. dev.      7931.077\n",
      "75%     15890.87       12703.74\n",
      "90%     23478.57       19077.99       Variance       6.29e+07\n",
      "95%     23548.21       23478.57       Skewness       .7700186\n",
      "99%     23548.21       23548.21       Kurtosis       2.121942\n",
      "\n",
      "                        Log Real GDP\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     7.714089       7.714089\n",
      " 5%     7.714089       7.714089\n",
      "10%     7.714089       8.052368       Obs                  12\n",
      "25%     8.123472       8.194575       Sum of wgt.          12\n",
      "\n",
      "50%     8.946424                      Mean           8.898784\n",
      "                        Largest       Std. dev.       .855707\n",
      "75%     9.652971       9.449652\n",
      "90%     10.06384       9.856291       Variance       .7322344\n",
      "95%      10.0668       10.06384       Skewness      -.0094643\n",
      "99%      10.0668        10.0668       Kurtosis        1.72738\n",
      "\n",
      "                 GDP Growth Rate (Quarterly)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.0820427      -.0820427\n",
      " 5%    -.0820427       -.001627\n",
      "10%     -.001627       .0031404       Obs                  12\n",
      "25%     .0042453       .0053501       Sum of wgt.          12\n",
      "\n",
      "50%     .0091004                      Mean            .002871\n",
      "                        Largest       Std. dev.      .0275393\n",
      "75%     .0157766       .0149479\n",
      "90%      .019022       .0166054       Variance       .0007584\n",
      "95%     .0193586        .019022       Skewness      -2.713874\n",
      "99%     .0193586       .0193586       Kurtosis       9.002529\n"
     ]
    }
   ],
   "source": [
    "* Add variable labels\n",
    "label var date \"Date (Monthly)\"\n",
    "label var cpi \"Consumer Price Index\"\n",
    "label var inflation \"Inflation Rate (Monthly % Change)\"\n",
    "label var interest_rate \"Interest Rate (%)\"\n",
    "label var unemployment \"Unemployment Rate (%)\"\n",
    "label var stock_return \"Stock Market Return (Monthly)\"\n",
    "label var gdp \"Real GDP (Quarterly, interpolated)\"\n",
    "label var lngdp \"Log Real GDP\"\n",
    "label var gdp_growth \"GDP Growth Rate (Quarterly)\"\n",
    "\n",
    "* Set time series\n",
    "tsset date\n",
    "\n",
    "* Check time series structure\n",
    "tsset\n",
    "\n",
    "* Display summary\n",
    "describe\n",
    "summarize, detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0429c5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "file data/processed/macro_finance_data.dta saved\n",
      "\n",
      "Macro-finance data processed and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "* Save processed macro-finance data\n",
    "save \"data/processed/macro_finance_data.dta\", replace\n",
    "\n",
    "display \"Macro-finance data processed and saved successfully!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15baeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Temporary files cleaned up.\n"
     ]
    }
   ],
   "source": [
    "* Clean up temporary files\n",
    "erase \"data/processed/temp_gdp.dta\"\n",
    "erase \"data/processed/temp_cpi.dta\"\n",
    "erase \"data/processed/temp_interest.dta\"\n",
    "erase \"data/processed/temp_unemployment.dta\"\n",
    "erase \"data/processed/temp_stock.dta\"\n",
    "erase \"data/processed/temp_gdp_monthly.dta\"\n",
    "\n",
    "display \"Temporary files cleaned up.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f7ee2",
   "metadata": {},
   "source": [
    "### Cleaning Up Temporary Files\n",
    "\n",
    "**The `erase` Command**\n",
    "\n",
    "**Syntax:** `erase filename`\n",
    "\n",
    "**What it does:**\n",
    "- Permanently deletes a file from disk\n",
    "- Cannot be undone (no recycle bin/trash in Stata)\n",
    "- Use with caution!\n",
    "\n",
    "**Why delete temporary files:**\n",
    "1. **Organization**: Keep processed folder clean (only final datasets)\n",
    "2. **Storage**: Free up disk space (though these files are small)\n",
    "3. **Clarity**: Prevents confusion about which files are final vs. intermediate\n",
    "4. **Reproducibility**: Temporary files can always be recreated by rerunning code\n",
    "\n",
    "**Best practices:**\n",
    "- Only erase files you can regenerate\n",
    "- Never erase raw data\n",
    "- Document what gets erased (as we do here)\n",
    "- Consider keeping temps during development, erase in final version\n",
    "\n",
    "**Alternative approach:** Save temps in separate folder\n",
    "```stata\n",
    "save \"data/temp/temp_gdp.dta\", replace\n",
    "```\n",
    "Then delete entire `temp/` folder when done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c7e1a",
   "metadata": {},
   "source": [
    "<style>\n",
    "h2.styled-header {\n",
    "    max-width: 600px;\n",
    "    margin: 20px auto 22px !important;\n",
    "    padding: 20px 32px !important;\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid #e5e7eb;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%);\n",
    "    box-shadow: 0 8px 26px rgba(0,0,0,0.06);\n",
    "    overflow: hidden;\n",
    "    text-align: center;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 800 !important;\n",
    "    color: #14276c !important;\n",
    "    margin-bottom: 8px !important;\n",
    "    margin-top: 0 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2 class=\"styled-header\">6) Data Quality Checks</h2>\n",
    "\n",
    "Perform basic quality checks on all processed datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430278a",
   "metadata": {},
   "source": [
    "## Why Data Quality Checks Are Essential\n",
    "\n",
    "After processing data, we must **validate** that everything worked correctly before using it in analysis.\n",
    "\n",
    "**What can go wrong in data processing:**\n",
    "1. **Date conversion errors**: Dates parsed incorrectly (e.g., DD/MM/YY vs MM/DD/YY)\n",
    "2. **Merge failures**: Variables don't align properly across datasets\n",
    "3. **Transformation errors**: Formulas applied incorrectly\n",
    "4. **Unexpected missing values**: Data gaps we didn't anticipate\n",
    "5. **Outliers or data errors**: Values that don't make economic sense\n",
    "6. **Time series gaps**: Broken temporal structure\n",
    "\n",
    "**The quality check workflow:**\n",
    "\n",
    "For each dataset, we check:\n",
    "1. **Time series structure** (`tsset`): Confirms proper temporal setup\n",
    "2. **Sample period**: Verify date range matches expectations\n",
    "3. **Missing values**: Count and understand why they're missing\n",
    "4. **Distribution**: Summary statistics to spot outliers or errors\n",
    "5. **Economic plausibility**: Values make sense given what we know about the data\n",
    "\n",
    "**Benefits:**\n",
    "- **Catch errors early**: Before they propagate through analysis\n",
    "- **Document data**: Know sample period, coverage, limitations\n",
    "- **Build confidence**: Verified data → reliable results\n",
    "- **Reproducibility**: Others can verify your data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Time variable: date, 04nov2015 to 03nov2025, but with gaps\n",
      "        Delta: 1 day\n",
      "\n",
      "=== S&P 500 Data Quality Check ===\n",
      "\n",
      "Date range: 04nov2015 to 03nov2025\n",
      "\n",
      "Number of observations: 2609\n",
      "\n",
      "  95\n",
      "\n",
      "  643\n",
      "\n",
      "\n",
      "                     S&P 500 Log Returns\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.0342684      -.0999451\n",
      " 5%    -.0172119      -.0616093\n",
      "10%    -.0111666      -.0607519       Obs               1,966\n",
      "25%     -.003767      -.0532222       Sum of wgt.       1,966\n",
      "\n",
      "50%     .0006955                      Mean           .0005213\n",
      "                        Largest       Std. dev.      .0112914\n",
      "75%     .0058107        .060544\n",
      "90%     .0117779       .0888085       Variance       .0001275\n",
      "95%     .0160189       .0896831       Skewness      -.0679821\n",
      "99%     .0258198       .0908947       Kurtosis       15.01661\n"
     ]
    }
   ],
   "source": [
    "* Check S&P 500 data\n",
    "use \"data/processed/sp500_data.dta\", clear\n",
    "tsset date\n",
    "\n",
    "display \"=== S&P 500 Data Quality Check ===\"\n",
    "display \"Date range: \" %td r(tmin) \" to \" %td r(tmax)\n",
    "display \"Number of observations: \" _N\n",
    "count if missing(sp500_close)\n",
    "count if missing(sp500_return)\n",
    "summarize sp500_return, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9aee0",
   "metadata": {},
   "source": [
    "### Quality Check: S&P 500 Data\n",
    "\n",
    "**Understanding `tsset` Output**\n",
    "\n",
    "When we run `tsset date` (with data already tsset), Stata displays:\n",
    "```\n",
    "Time variable: date, 03jan1950 to 31dec2023\n",
    "        Delta: 1 day\n",
    "```\n",
    "\n",
    "**What to verify:**\n",
    "- **Time variable**: Should be `date` (our standardized name)\n",
    "- **Range**: Check start and end dates\n",
    "  - S&P 500 available from 1950s\n",
    "  - End date should be recent (when data was downloaded)\n",
    "- **Delta**: Should be 1 day for daily data\n",
    "- **Format**: Should show `%td` (daily format)\n",
    "\n",
    "**Using `r()` return values:**\n",
    "\n",
    "After `tsset`, Stata stores information in return values:\n",
    "- `r(tmin)`: First time value (numeric)\n",
    "- `r(tmax)`: Last time value (numeric)\n",
    "- `r(tdelta)`: Time between observations\n",
    "\n",
    "**The `display` command with format:**\n",
    "```stata\n",
    "display \"Date range: \" %td r(tmin) \" to \" %td r(tmax)\n",
    "```\n",
    "\n",
    "- `%td`: Formats the numeric date as a readable daily date\n",
    "- Without `%td`: Would show numbers like 0 (Jan 1, 1960)\n",
    "- With `%td`: Shows \"03jan1950\", \"31dec2023\"\n",
    "\n",
    "**Checking observation count:**\n",
    "- `_N`: System variable = total number of observations\n",
    "- For daily data: approximately 252 trading days/year\n",
    "- Example: 1950-2023 (73 years) × 252 ≈ 18,400 observations\n",
    "\n",
    "**Counting missing values:**\n",
    "```stata\n",
    "count if missing(sp500_close)\n",
    "count if missing(sp500_return)\n",
    "```\n",
    "\n",
    "**Expected:**\n",
    "- `sp500_close`: Few missing (maybe 0 if data is complete)\n",
    "- `sp500_return`: At least 1 missing (first observation has no previous price)\n",
    "\n",
    "**Additional missing possible:**\n",
    "- Market closures (9/11 attacks, Hurricane Sandy)\n",
    "- Data gaps from source\n",
    "- Should be small number\n",
    "\n",
    "**Interpreting summary statistics:**\n",
    "\n",
    "For `sp500_return` (daily log returns):\n",
    "- **Mean**: Should be small positive (≈ 0.03-0.05% per day → 8-13% annual)\n",
    "- **Std Dev**: Should be ≈ 1-1.5% (daily volatility)\n",
    "- **Min**: Extreme crashes (Black Monday 1987: -20%, COVID: -12%)\n",
    "- **Max**: Extreme rallies (usually < +10%)\n",
    "- **Skewness**: Slightly negative (crashes bigger than rallies)\n",
    "- **Kurtosis**: > 3 (fat tails, extreme events more common than normal)\n",
    "\n",
    "**Red flags:**\n",
    "- Mean too large (> 0.5% daily): Possible data error\n",
    "- Std Dev too small (< 0.5%): Missing volatility data\n",
    "- Returns > 50%: Likely data error or stock split not adjusted\n",
    "- Too many zeros: Stale prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703ed92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Time variable: date, 04jan1999 to 31oct2025, but with gaps\n",
      "        Delta: 1 day\n",
      "\n",
      "=== EUR/USD Data Quality Check ===\n",
      "\n",
      "Date range: 04jan1999 to 31oct2025\n",
      "\n",
      "Number of observations: 7000\n",
      "\n",
      "  270\n",
      "\n",
      "  270\n",
      "\n",
      "  1,742\n",
      "\n",
      "\n",
      "                    EUR/USD Exchange Rate\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%        .8598           .827\n",
      " 5%        .9037          .8271\n",
      "10%       .98485          .8364       Obs               6,730\n",
      "25%       1.0844          .8364       Sum of wgt.       6,730\n",
      "\n",
      "50%       1.1702                      Mean           1.182768\n",
      "                        Largest       Std. dev.      .1540254\n",
      "75%       1.2994         1.5923\n",
      "90%      1.37945         1.5924       Variance       .0237238\n",
      "95%       1.4399         1.5978       Skewness       .1009166\n",
      "99%       1.5569          1.601       Kurtosis       2.676944\n",
      "\n",
      "                  Log EUR/USD Exchange Rate\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.1510555      -.1899506\n",
      " 5%    -.1012578      -.1898297\n",
      "10%    -.0152659      -.1786483       Obs               6,730\n",
      "25%     .0810269      -.1786483       Sum of wgt.       6,730\n",
      "\n",
      "50%     .1571747                      Mean           .1592579\n",
      "                        Largest       Std. dev.      .1318484\n",
      "75%     .2619026       .4651796\n",
      "90%     .3216849       .4652423       Variance        .017384\n",
      "95%     .3645737       .4686277       Skewness      -.2322385\n",
      "99%     .4426967       .4706284       Kurtosis       2.770172\n",
      "\n",
      "               First Difference of Log EUR/USD\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     -.015197       -.030031\n",
      " 5%    -.0095197      -.0267241\n",
      "10%    -.0067176      -.0219574       Obs               5,258\n",
      "25%    -.0031692      -.0219432       Sum of wgt.       5,258\n",
      "\n",
      "50%     -.000073                      Mean           .0000154\n",
      "                        Largest       Std. dev.      .0058038\n",
      "75%     .0031839       .0296149\n",
      "90%     .0069673       .0306427       Variance       .0000337\n",
      "95%     .0094788       .0389142       Skewness       .1440726\n",
      "99%     .0145946        .046208       Kurtosis       5.551395\n"
     ]
    }
   ],
   "source": [
    "* Check EUR/USD data\n",
    "use \"data/processed/eurusd_data.dta\", clear\n",
    "tsset date\n",
    "\n",
    "display \"=== EUR/USD Data Quality Check ===\"\n",
    "display \"Date range: \" %td r(tmin) \" to \" %td r(tmax)\n",
    "display \"Number of observations: \" _N\n",
    "count if missing(eurusd)\n",
    "count if missing(lneurusd)\n",
    "count if missing(d_lneurusd)\n",
    "summarize eurusd lneurusd d_lneurusd, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b5c42",
   "metadata": {},
   "source": [
    "### Quality Check: EUR/USD Data\n",
    "\n",
    "**What to verify:**\n",
    "\n",
    "**Time series structure:**\n",
    "- Delta: 1 day (daily exchange rate)\n",
    "- Range: EUR introduced January 1, 1999\n",
    "  - Before 1999: synthetic EUR (weighted basket of predecessor currencies)\n",
    "  - FRED typically starts 1999-01-04 (first trading day)\n",
    "\n",
    "**Expected missing values:**\n",
    "\n",
    "Similar to stock data:\n",
    "- `eurusd`: Few missing (weekends/holidays)\n",
    "- `lneurusd`: Same as eurusd (log of non-missing = non-missing)\n",
    "- `d_lneurusd`: One additional missing (first observation)\n",
    "\n",
    "**Number of observations:**\n",
    "- 1999-2023: 24 years × 252 ≈ 6,000 trading days\n",
    "- Actual slightly less due to holidays\n",
    "\n",
    "**Interpreting exchange rate statistics:**\n",
    "\n",
    "**For `eurusd` (level):**\n",
    "- **Range**: Historically 0.85 to 1.60\n",
    "  - Low point: ~0.85 (2000-2001, strong USD)\n",
    "  - High point: ~1.60 (2008, weak USD)\n",
    "  - Recent: ~1.00-1.20\n",
    "- **Mean**: Around 1.15-1.20 (over full sample)\n",
    "- **Non-stationary**: Level has trend/drift\n",
    "\n",
    "**For `lneurusd` (log level):**\n",
    "- **Range**: ln(0.85) ≈ -0.16 to ln(1.60) ≈ 0.47\n",
    "- **Mean**: Around ln(1.15) ≈ 0.14\n",
    "- Similar non-stationarity as level\n",
    "\n",
    "**For `d_lneurusd` (daily change / return):**\n",
    "- **Mean**: Should be ≈ 0 (no persistent trend)\n",
    "- **Std Dev**: ≈ 0.6-0.8% (daily FX volatility)\n",
    "  - Lower than stocks (≈1.5%)\n",
    "  - FX generally less volatile than equities on daily basis\n",
    "- **Skewness**: Close to 0 (symmetric)\n",
    "- **Kurtosis**: > 3 (fat tails, FX jumps on news)\n",
    "- **Min/Max**: Usually ±3-4% (extreme days)\n",
    "  - Brexit referendum: ~8% move\n",
    "  - SNB franc de-peg 2015: ~15% move\n",
    "\n",
    "**Red flags:**\n",
    "- Level outside 0.8-1.7 range: Likely error\n",
    "- Daily change > 10%: Check if real event or data error\n",
    "- Mean change far from 0: Suggests trend (unusual for FX)\n",
    "- Std dev > 2%: Too volatile for EUR/USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb44842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Time variable: date, 1947m1 to 2025m11\n",
      "        Delta: 1 month\n",
      "\n",
      "=== Macro-Finance Data Quality Check ===\n",
      "\n",
      "Date range:  1947m1 to 2025m11\n",
      "\n",
      "Number of observations: 947\n",
      "\n",
      "Missing values per variable:\n",
      "\n",
      "  2\n",
      "\n",
      "  CPI: 2\n",
      "\n",
      "  3\n",
      "\n",
      "  Inflation: 3\n",
      "\n",
      "  91\n",
      "\n",
      "  Interest Rate: 91\n",
      "\n",
      "  15\n",
      "\n",
      "  Unemployment: 15\n",
      "\n",
      "  826\n",
      "\n",
      "  Stock Return: 826\n",
      "\n",
      "  935\n",
      "\n",
      "  GDP: 935\n",
      "\n",
      "\n",
      "                    Consumer Price Index\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%        22.91          21.48\n",
      " 5%        24.98          21.62\n",
      "10%        26.85          21.95       Obs                 945\n",
      "25%        32.75             22       Sum of wgt.         945\n",
      "\n",
      "50%        109.5                      Mean           123.4215\n",
      "                        Largest       Std. dev.      88.82391\n",
      "75%        199.3          321.5\n",
      "90%      247.284        322.132       Variance       7889.687\n",
      "95%      276.528        323.364       Skewness       .4530693\n",
      "99%      317.603        324.368       Kurtosis       1.927122\n",
      "\n",
      "                       Date (Monthly)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%         -147           -156\n",
      " 5%         -109           -155\n",
      "10%          -62           -154       Obs                 947\n",
      "25%           80           -153       Sum of wgt.         947\n",
      "\n",
      "50%          317                      Mean                317\n",
      "                        Largest       Std. dev.      273.5197\n",
      "75%          554            787\n",
      "90%          696            788       Variance          74813\n",
      "95%          743            789       Skewness              0\n",
      "99%          781            790       Kurtosis       1.799997\n",
      "\n",
      "              Inflation Rate (Monthly % Change)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.5034924      -1.786423\n",
      " 5%    -.1638889      -.9239912\n",
      "10%    -.0447273      -.8635521       Obs                 944\n",
      "25%     .1005888      -.8267403       Sum of wgt.         944\n",
      "\n",
      "50%     .2473474                      Mean           .2875801\n",
      "                        Largest       Std. dev.      .3398177\n",
      "75%     .4373789       1.742363\n",
      "90%      .676918       1.757526       Variance       .1154761\n",
      "95%     .9370804       1.793766       Skewness       .5660575\n",
      "99%     1.382828       1.945233       Kurtosis       7.096109\n",
      "\n",
      "                      Interest Rate (%)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%          .07            .05\n",
      " 5%          .09            .05\n",
      "10%          .16            .06       Obs                 856\n",
      "25%        1.895            .07       Sum of wgt.         856\n",
      "\n",
      "50%         4.33                      Mean           4.605678\n",
      "                        Largest       Std. dev.      3.546838\n",
      "75%        6.135           18.9\n",
      "90%         9.24          19.04       Variance       12.58006\n",
      "95%        10.78          19.08       Skewness       1.073105\n",
      "99%        15.93           19.1       Kurtosis       4.691378\n",
      "\n",
      "                    Unemployment Rate (%)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%          2.9            2.5\n",
      " 5%          3.5            2.5\n",
      "10%          3.7            2.6       Obs                 932\n",
      "25%          4.3            2.6       Sum of wgt.         932\n",
      "\n",
      "50%          5.5                      Mean           5.670386\n",
      "                        Largest       Std. dev.      1.707418\n",
      "75%          6.7           10.8\n",
      "90%          7.8             11       Variance       2.915277\n",
      "95%            9           13.2       Skewness       .8738871\n",
      "99%         10.2           14.8       Kurtosis       4.073464\n",
      "\n",
      "                Stock Market Return (Monthly)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     -.100915      -.1157656\n",
      " 5%    -.0583563       -.100915\n",
      "10%     -.043582       -.088582       Obs                 121\n",
      "25%    -.0116663      -.0724497       Sum of wgt.         121\n",
      "\n",
      "50%     .0108213                      Mean           .0084696\n",
      "                        Largest       Std. dev.      .0380827\n",
      "75%     .0283365       .0785065\n",
      "90%     .0627522       .0787125       Variance       .0014503\n",
      "95%     .0658751       .0948958       Skewness      -.4427581\n",
      "99%     .0948958       .1042986       Kurtosis       3.897131\n",
      "\n",
      "             Real GDP (Quarterly, interpolated)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     2239.682       2239.682\n",
      " 5%     2239.682       2239.682\n",
      "10%     2239.682       3141.224       Obs                  12\n",
      "25%     3381.238       3621.252       Sum of wgt.          12\n",
      "\n",
      "50%     7688.202                      Mean            10021.6\n",
      "                        Largest       Std. dev.      7931.077\n",
      "75%     15890.87       12703.74\n",
      "90%     23478.57       19077.99       Variance       6.29e+07\n",
      "95%     23548.21       23478.57       Skewness       .7700186\n",
      "99%     23548.21       23548.21       Kurtosis       2.121942\n",
      "\n",
      "                        Log Real GDP\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%     7.714089       7.714089\n",
      " 5%     7.714089       7.714089\n",
      "10%     7.714089       8.052368       Obs                  12\n",
      "25%     8.123472       8.194575       Sum of wgt.          12\n",
      "\n",
      "50%     8.946424                      Mean           8.898784\n",
      "                        Largest       Std. dev.       .855707\n",
      "75%     9.652971       9.449652\n",
      "90%     10.06384       9.856291       Variance       .7322344\n",
      "95%      10.0668       10.06384       Skewness      -.0094643\n",
      "99%      10.0668        10.0668       Kurtosis        1.72738\n",
      "\n",
      "                 GDP Growth Rate (Quarterly)\n",
      "-------------------------------------------------------------\n",
      "      Percentiles      Smallest\n",
      " 1%    -.0820427      -.0820427\n",
      " 5%    -.0820427       -.001627\n",
      "10%     -.001627       .0031404       Obs                  12\n",
      "25%     .0042453       .0053501       Sum of wgt.          12\n",
      "\n",
      "50%     .0091004                      Mean            .002871\n",
      "                        Largest       Std. dev.      .0275393\n",
      "75%     .0157766       .0149479\n",
      "90%      .019022       .0166054       Variance       .0007584\n",
      "95%     .0193586        .019022       Skewness      -2.713874\n",
      "99%     .0193586       .0193586       Kurtosis       9.002529\n"
     ]
    }
   ],
   "source": [
    "* Check Macro-Finance data\n",
    "use \"data/processed/macro_finance_data.dta\", clear\n",
    "tsset date\n",
    "\n",
    "display \"=== Macro-Finance Data Quality Check ===\"\n",
    "display \"Date range: \" %tm r(tmin) \" to \" %tm r(tmax)\n",
    "display \"Number of observations: \" _N\n",
    "display \"Missing values per variable:\"\n",
    "count if missing(cpi)\n",
    "display \"  CPI: \" r(N)\n",
    "count if missing(inflation)\n",
    "display \"  Inflation: \" r(N)\n",
    "count if missing(interest_rate)\n",
    "display \"  Interest Rate: \" r(N)\n",
    "count if missing(unemployment)\n",
    "display \"  Unemployment: \" r(N)\n",
    "count if missing(stock_return)\n",
    "display \"  Stock Return: \" r(N)\n",
    "count if missing(gdp)\n",
    "display \"  GDP: \" r(N)\n",
    "\n",
    "summarize, detail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4201e9f",
   "metadata": {},
   "source": [
    "### Quality Check: Macro-Finance Data\n",
    "\n",
    "**What to verify:**\n",
    "\n",
    "**Time series structure:**\n",
    "- Delta: 1 month (monthly data)\n",
    "- Format: `%tm` (monthly format like \"2020m3\")\n",
    "- Range: Limited by least available series\n",
    "  - CPI: 1947-present\n",
    "  - Fed Funds: 1954-present  \n",
    "  - Others: 1948-present\n",
    "  - **Effective start**: When ALL series available (≈1954)\n",
    "\n",
    "**Display format for monthly dates:**\n",
    "```stata\n",
    "display \"Date range: \" %tm r(tmin) \" to \" %tm r(tmax)\n",
    "```\n",
    "- `%tm`: Formats numeric monthly date as \"1954m7\", \"2023m12\"\n",
    "\n",
    "**Expected number of observations:**\n",
    "- Sample: 1954-2023 → 69 years × 12 months = 828 observations\n",
    "- Actual may be slightly less if recent months incomplete\n",
    "\n",
    "---\n",
    "\n",
    "**Checking Missing Values**\n",
    "\n",
    "Missing values can occur for several reasons:\n",
    "\n",
    "**1. Series start at different dates** (most common):\n",
    "- If we include data from 1947, but Fed Funds starts 1954:\n",
    "  - 1947m1 to 1954m6: CPI, GDP, etc. present; interest_rate missing\n",
    "  - After 1954m7: All variables present\n",
    "\n",
    "**2. Revisions and releases:**\n",
    "- Most recent months might miss GDP (released with lag)\n",
    "- Recent CPI might be preliminary\n",
    "\n",
    "**3. Historical data gaps:**\n",
    "- WWII period: Some series interrupted\n",
    "- Methodological changes: Series redefined\n",
    "\n",
    "**4. Our processing:**\n",
    "- First observation of growth rates/changes always missing\n",
    "- But we created these from levels, so minimal impact\n",
    "\n",
    "**Interpreting the counts:**\n",
    "```stata\n",
    "count if missing(cpi)\n",
    "display \"  CPI: \" r(N)\n",
    "```\n",
    "\n",
    "**Expected patterns:**\n",
    "- **CPI, inflation**: Should be nearly complete (≈0 missing if 1954+)\n",
    "- **Interest rates**: Might have a few early or recent missing\n",
    "- **Unemployment**: Should be complete\n",
    "- **Stock returns**: First month missing (need previous month for return)\n",
    "- **GDP, gdp_growth**: \n",
    "  - Level (gdp, lngdp): Might miss very recent months\n",
    "  - Growth: Additionally misses first quarter\n",
    "\n",
    "---\n",
    "\n",
    "**Interpreting Summary Statistics**\n",
    "\n",
    "**Expected ranges for each variable:**\n",
    "\n",
    "**CPI (level):**\n",
    "- 1954: ≈27 (base 1982-84=100)\n",
    "- 2023: ≈300\n",
    "- Always increasing (except rare deflation)\n",
    "\n",
    "**Inflation (monthly %):**\n",
    "- Mean: ≈0.2-0.3% monthly (3-4% annualized)\n",
    "- Range: -2% to +2% monthly\n",
    "- Higher in 1970s-80s, lower in 1990s-2000s\n",
    "\n",
    "**Interest rate (%):**\n",
    "- Mean: ≈4-5% (over full sample)\n",
    "- Range: 0% (2008-2015, 2020-2021) to 20% (1980-1981)\n",
    "- Highly variable across monetary regimes\n",
    "\n",
    "**Unemployment (%):**\n",
    "- Mean: ≈5-6%\n",
    "- Range: 2.5% (1953) to 14.7% (2020)\n",
    "- Cyclical: rises in recessions\n",
    "\n",
    "**Stock return (monthly %):**\n",
    "- Mean: ≈0.8-1.0% monthly (≈10-12% annualized)\n",
    "- Std Dev: ≈4-5% monthly\n",
    "- Range: -20% to +15% (extreme months)\n",
    "\n",
    "**GDP (billions 2017$):**\n",
    "- 1954: ≈3,000\n",
    "- 2023: ≈22,000\n",
    "- Steady growth with business cycles\n",
    "\n",
    "**GDP growth (quarterly %):**\n",
    "- Mean: ≈0.6-0.8% quarterly (≈3% annualized)\n",
    "- Range: -10% (COVID) to +8% (post-recession bouncebacks)\n",
    "- Note: Carried forward to all 3 months, so appears very persistent\n",
    "\n",
    "---\n",
    "\n",
    "**What makes this dataset special:**\n",
    "\n",
    "This is a **macro-financial VAR dataset** containing:\n",
    "- **Real economy**: GDP, unemployment (quantity variables)\n",
    "- **Prices**: Inflation (price level changes)\n",
    "- **Monetary policy**: Interest rates (policy instrument)\n",
    "- **Financial markets**: Stock returns (risk asset performance)\n",
    "\n",
    "**Potential analyses:**\n",
    "- Impulse responses: How does GDP respond to interest rate shocks?\n",
    "- Variance decomposition: What drives inflation variation?\n",
    "- Granger causality: Do stock returns predict GDP?\n",
    "- Policy evaluation: Effectiveness of monetary policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b070b",
   "metadata": {},
   "source": [
    "<style>\n",
    "h2.styled-header {\n",
    "    max-width: 600px;\n",
    "    margin: 20px auto 22px !important;\n",
    "    padding: 20px 32px !important;\n",
    "    border-radius: 18px;\n",
    "    border: 1px solid #e5e7eb;\n",
    "    background: linear-gradient(180deg, #ffffff 0%, #f9fafb 100%);\n",
    "    box-shadow: 0 8px 26px rgba(0,0,0,0.06);\n",
    "    overflow: hidden;\n",
    "    text-align: center;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: 800 !important;\n",
    "    color: #14276c !important;\n",
    "    margin-bottom: 8px !important;\n",
    "    margin-top: 0 !important;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<h2 class=\"styled-header\">7) Summary and Next Steps</h2>\n",
    "\n",
    "## What We Accomplished\n",
    "\n",
    "This notebook has transformed **raw CSV files from FRED** into **analysis-ready Stata datasets**. Here's what we've done:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. S&P 500 Data (`sp500_data.dta`)**\n",
    "\n",
    "**Processing steps:**\n",
    "- ✓ Imported daily price data\n",
    "- ✓ Converted string dates to Stata daily format (`%td`)\n",
    "- ✓ Computed log returns: $r_t = \\ln(P_t) - \\ln(P_{t-1})$\n",
    "- ✓ Set time series structure with `tsset`\n",
    "- ✓ Added descriptive variable labels\n",
    "\n",
    "**What's ready:**\n",
    "- Daily returns for **ARMA modeling** (autocorrelation structure)\n",
    "- Returns for **GARCH models** (volatility clustering)\n",
    "- Sample for **unit root tests** (random walk hypothesis)\n",
    "\n",
    "**Key variables:**\n",
    "- `date`: Daily date (trading days only)\n",
    "- `sp500_close`: Closing price (level)\n",
    "- `sp500_return`: Log returns (%)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. EUR/USD Data (`eurusd_data.dta`)**\n",
    "\n",
    "**Processing steps:**\n",
    "- ✓ Imported daily exchange rate data\n",
    "- ✓ Converted dates to daily format\n",
    "- ✓ Created log transformation: $\\ln(S_t)$\n",
    "- ✓ Computed first difference: $\\Delta \\ln(S_t)$\n",
    "- ✓ Set time series structure\n",
    "\n",
    "**What's ready:**\n",
    "- Levels for **unit root testing** (Dickey-Fuller, Phillips-Perron)\n",
    "- First differences for **stationarity tests**\n",
    "- Exchange rate returns for **forecasting models**\n",
    "\n",
    "**Key variables:**\n",
    "- `date`: Daily date\n",
    "- `eurusd`: Exchange rate level (USD per EUR)\n",
    "- `lneurusd`: Log exchange rate\n",
    "- `d_lneurusd`: First difference of log rate (≈ returns)\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Macro-Finance Data (`macro_finance_data.dta`)**\n",
    "\n",
    "**Processing steps:**\n",
    "- ✓ Processed 5 separate economic time series\n",
    "- ✓ Converted quarterly GDP → monthly (repeated values)\n",
    "- ✓ Aggregated daily stock returns → monthly\n",
    "- ✓ Merged all series on monthly dates\n",
    "- ✓ Created transformations (inflation, GDP growth)\n",
    "- ✓ Set monthly time series structure\n",
    "\n",
    "**What's ready:**\n",
    "- Complete dataset for **Vector Autoregression (VAR)**\n",
    "- Variables for **impulse response analysis**\n",
    "- Data for **Granger causality tests**\n",
    "- Series for **cointegration analysis**\n",
    "\n",
    "**Key variables:**\n",
    "- `date`: Monthly date (`%tm` format)\n",
    "- `cpi`: Consumer Price Index (level)\n",
    "- `inflation`: Monthly inflation rate (%)\n",
    "- `interest_rate`: Federal Funds Rate (%)\n",
    "- `unemployment`: Unemployment rate (%)\n",
    "- `stock_return`: Monthly S&P 500 return (%)\n",
    "- `gdp`: Real GDP (billions 2017$, quarterly repeated monthly)\n",
    "- `lngdp`: Log real GDP\n",
    "- `gdp_growth`: Quarterly GDP growth rate (%)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Stata Skills Learned\n",
    "\n",
    "**Data Import:**\n",
    "- `import delimited`: Load CSV files\n",
    "- `describe`, `list`: Inspect data structure\n",
    "\n",
    "**Date Handling:**\n",
    "- `date()`, `ym()`, `yq()`: Parse dates\n",
    "- `%td`, `%tm`, `%tq`: Date formats\n",
    "- `year()`, `month()`, `quarter()`: Extract date components\n",
    "\n",
    "**Time Series Setup:**\n",
    "- `tsset`: Declare time series structure\n",
    "- `L.`, `F.`, `D.`: Lag, lead, difference operators\n",
    "\n",
    "**Data Transformation:**\n",
    "- `gen`, `egen`: Create variables\n",
    "- `ln()`: Natural logarithm\n",
    "- `bysort`: Operations by group\n",
    "- `expand`: Duplicate observations\n",
    "\n",
    "**Data Management:**\n",
    "- `rename`: Change variable names\n",
    "- `label var`: Add descriptive labels\n",
    "- `drop`, `keep`: Select variables/observations\n",
    "- `duplicates`: Handle duplicate observations\n",
    "- `sort`: Order data\n",
    "\n",
    "**Merging:**\n",
    "- `merge 1:1`: One-to-one merge\n",
    "- Understanding merge types and `_merge` variable\n",
    "\n",
    "**File Operations:**\n",
    "- `save`, `use`: Save and load `.dta` files\n",
    "- `erase`: Delete files\n",
    "\n",
    "**Validation:**\n",
    "- `summarize`: Summary statistics\n",
    "- `count if missing()`: Check missing values\n",
    "- `_N`, `_n`: Observation counters\n",
    "\n",
    "---\n",
    "\n",
    "## Important Concepts Learned\n",
    "\n",
    "### **Statistical/Econometric:**\n",
    "1. **Stationarity**: Why we difference and take logs\n",
    "2. **Returns vs. Prices**: Time series properties differ\n",
    "3. **Log transformation**: Stabilizes variance, interpretable as %\n",
    "4. **Integration**: $I(0)$ vs. $I(1)$ processes\n",
    "5. **Frequency conversion**: Quarterly → Monthly challenges\n",
    "\n",
    "### **Time Series Theory:**\n",
    "- Unit roots and random walks\n",
    "- Autocorrelation in financial data\n",
    "- Volatility clustering\n",
    "- Business cycle persistence\n",
    "- Policy transmission mechanisms\n",
    "\n",
    "### **Data Management:**\n",
    "- Raw vs. processed data separation\n",
    "- Reproducible workflows\n",
    "- Data validation importance\n",
    "- Temporary files strategy\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps in Subsequent Notebooks\n",
    "\n",
    "### **Session 1, Notebook 1: ARMA and Unit Roots**\n",
    "- Use `sp500_data.dta` and `eurusd_data.dta`\n",
    "- Estimate ARMA models for returns\n",
    "- Conduct unit root tests (ADF, PP)\n",
    "- Test random walk hypothesis\n",
    "\n",
    "### **Session 1, Notebook 2: VAR and Cointegration**\n",
    "- Use `macro_finance_data.dta`\n",
    "- Estimate Vector Autoregression\n",
    "- Compute impulse response functions\n",
    "- Test for cointegration (Johansen test)\n",
    "\n",
    "### **Session 2: Advanced Models**\n",
    "- GARCH models for volatility (S&P 500)\n",
    "- State space and Kalman filter (macro data)\n",
    "- Regime switching models\n",
    "- Extreme value theory\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices Demonstrated\n",
    "\n",
    "1. **Always start with `clear all`** - Clean workspace\n",
    "2. **Document with comments** - Explain what code does\n",
    "3. **Check data at each step** - `list`, `describe`, `summarize`\n",
    "4. **Use descriptive variable names** - `sp500_close` not `price`\n",
    "5. **Add variable labels** - Permanent documentation\n",
    "6. **Validate after processing** - Quality checks catch errors\n",
    "7. **Separate raw and processed** - Never modify raw data\n",
    "8. **Use `tsset` before time series operations** - Required for operators\n",
    "9. **Handle missing values explicitly** - Use `missing()` function\n",
    "10. **Keep code reproducible** - Can rerun from raw data\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls to Avoid\n",
    "\n",
    "**❌ Don't:**\n",
    "- Use `== .` for missing values (use `missing()` instead)\n",
    "- Forget `tsset` before using `L.`, `D.`, etc.\n",
    "- Mix up difference-log and log-difference\n",
    "- Ignore missing value patterns\n",
    "- Assume dates parse correctly (always verify!)\n",
    "- Merge without checking `_merge` results\n",
    "- Delete raw data files\n",
    "\n",
    "**✓ Do:**\n",
    "- Always verify date conversions with `list`\n",
    "- Check for duplicates before `tsset`\n",
    "- Use `describe` and `summarize` liberally\n",
    "- Keep intermediate steps when debugging\n",
    "- Document units (%, levels, logs)\n",
    "- Save processed data with clear names\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset Availability\n",
    "\n",
    "All processed datasets are now in `data/processed/`:\n",
    "- ✓ `sp500_data.dta` - Ready for ARMA/GARCH\n",
    "- ✓ `eurusd_data.dta` - Ready for unit root tests\n",
    "- ✓ `macro_finance_data.dta` - Ready for VAR\n",
    "\n",
    "**You can load them anytime:**\n",
    "```stata\n",
    "use \"data/processed/sp500_data.dta\", clear\n",
    "use \"data/processed/eurusd_data.dta\", clear\n",
    "use \"data/processed/macro_finance_data.dta\", clear\n",
    "```\n",
    "\n",
    "**All datasets are:**\n",
    "- Properly `tsset`\n",
    "- Labeled and documented\n",
    "- Quality-checked\n",
    "- Ready for econometric analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've successfully cleaned and prepared three time series datasets using professional workflows. These skills transfer to any time series project:\n",
    "- Financial data analysis\n",
    "- Macroeconomic forecasting  \n",
    "- Policy evaluation\n",
    "- Academic research\n",
    "\n",
    "**Now you're ready to start modeling!** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Stata",
   "language": "stata",
   "name": "stata"
  },
  "language_info": {
   "codemirror_mode": "stata",
   "file_extension": ".do",
   "mimetype": "text/x-stata",
   "name": "stata",
   "version": "15.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
